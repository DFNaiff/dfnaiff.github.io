<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Visualizing Layer Normalization | Danilo Naiff</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Visualizing Layer Normalization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4000/2023/01/15/visualizing-layer-normalization.html" />
<meta property="og:url" content="http://localhost:4000/2023/01/15/visualizing-layer-normalization.html" />
<meta property="og:site_name" content="Danilo Naiff" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-15T17:49:16-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Visualizing Layer Normalization" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-01-15T17:49:16-03:00","datePublished":"2023-01-15T17:49:16-03:00","description":"Introduction","headline":"Visualizing Layer Normalization","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/01/15/visualizing-layer-normalization.html"},"url":"http://localhost:4000/2023/01/15/visualizing-layer-normalization.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Danilo Naiff" />

<!--
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
-->
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  },
  TeX: {
    Macros: {
      ket: ["\\left| #1 \\right\\rangle", 1],
      bra: ["\\left\\langle #1 \\right|", 1],
      braket: ["\\left\\langle #1 \\middle| #2 \\right\\rangle", 2],
      mean: ["\\left\\langle #1 \\right\\rangle", 1],
      abs: ["\\left| #1 \\right|", 1],
      norm: ["\\left\\| #1 \\right\\|", 1],
      trace: ["\\text{tr}\\left( #1 \\right)", 1],
      ptrace: ["\\text{tr}\_{#1}\\left( #2 \\right)", 2],
      pderiv: ["\\frac{\\partial #1}{\\partial #2}", 2],
      evalat: ["\\left. #1 \\right|_{#2}", 2],
    }
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script></head>
<body><header class="site-header">

  <div class="wrapper">
    <!--<a class="site-title" rel="author" href="/">Danilo Naiff</a>
    --><nav class="navbar navbar-expand-lg navbar-light bg-light">
      <a class="navbar-brand" href="/">Danilo Naiff</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
              Apps
            </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
              <a class="dropdown-item" href="/golf">Gravitational Golf</a>
              <a class="dropdown-item" href="/mbs">Quasielectrostatics</a>
            </div>
          </li>
        </ul>
      </div>
    </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Visualizing Layer Normalization</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-01-15T17:49:16-03:00" itemprop="datePublished">
        Jan 15, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>Here, I explore some geometric intuition of layer normalization, that are found in transformers. I’ve been in general trying to “imagine” operations on transformers. My “visualization” of multi-head attention largely follows the insights from <a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</a>, while I imagine the feedforward neural network part as “warping the space” of embedding vectors. I struggled a bit with layer normalization until I realized that it had a quite nice interpretation. I’m sharing this here in case anyone is interested.</p>

<h1 id="visualizing-layer-normalization">Visualizing layer normalization</h1>

<p>In a neural network, the layer norm operation on a vector \(x \in \mathbb{R}^n\) is the operation defined by</p>

\[\operatorname{lnorm}(x;a,b) = a \odot \frac{x - \operatorname{mean}(x)}{\operatorname{std}(x) + \epsilon} + b \\ \quad \\
\operatorname{mean}(x) = \frac{1}{n} \sum_{i=1}^n x_i \\ \quad \\
\operatorname{std}(x) = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \operatorname{mean}(x))^2}\]

<p>where \(a, b \in \mathbb{R}^n\) are parameters of the layer norm, and \(\epsilon\) is an small stability factor (for instance \(\epsilon = 10^{-12}\) is valid). Here, \(\odot\) denotes the element-wise product. Layer normalization is more interpretable if it can be seen as the composition of three functions</p>

\[\operatorname{lnorm}(x;a,b) = h(g(f(x));a, b) \\
\quad\\
f(x) = \left(I_{n} - \frac{1}{n} \mathbf{1}_{n \times n} \right) x \\
\quad \\
g(x) = \sqrt{n} \frac{x}{\|x\|_2 + \epsilon \sqrt{n}} \\
\quad \\
h(x;a,b) = \operatorname{diag}(a)x+b ,\]

<p>where \(I_{n}\) is the \(n\)-dimensional identity matrix, and \(\mathbf{1}_{n \times n}\) is the \(n\)-dimensional matrix of ones. We can interpret each of these in turn.</p>

<p>We begin with \(f\). First, consider the matrix \(P = \frac{1}{n} \mathbf{1}_{n \times n}\). It is not hard to see both the vector of ones \(\mathbf{1}_n\) is an eigenvector of \(P\) with eigenvalue \(\lambda_1 = 1\). Moreover, the null space of \(P\) must be of dimension \(n-1\), so it forms an eigenspace with eigenvalue being 0, that is orthogonal to \(\mathbf{1}_n\) (since \(P\) is symmetric). Therefore, \(P\) is an orthogonal on the 1-dimensional subspace \(\{x; x_1=x_2=\ldots=x_n\}\), generated by \(\mathbf{1}_n\). It follows that \(I - P\) is an orthogonal projection on the \((n-1)\)-dimensional subspace orthogonal to \(\mathbf{1}_n\), which we denote by \(U\). Therefore, \(f\) is an orthogonal projection into \(U\).</p>

<p>Going on with \(g\), we have that, if \(\|x\|_2 \gg \epsilon\), which will be true except for vectors very close to the origin, \(g = \sqrt{n} x/\|x\|_2\) is just a projection into a \((n-1)\)-dimensional sphere of radius \(\sqrt{n}\), which we denote \(\sqrt{n}S_{n-1} \in \mathbb{R}^n\). Since \(f(x)\) already projects \(x\) to \(U\), it means that \(g(f(x))\) actually lives on an \((n-2)\)-dimensional sphere contained in \(U\). We denote this sphere by \(\sqrt{n} S_{n-2, U} = S_{n-1} \cap U\).</p>

<p>Finally, \(h(x; a, b)\) is an scaling by \(A = \operatorname{diag}{a}\) and change of location by \(b\). Notice that, since our vectors \(x\) are living in \(\sqrt{n} S_{n-2, U}\) after the composition \(g \circ f\), \(A\) not only distorts \(\sqrt{n} S_{n-2, U}\) into an elipsis, but actually takes \(U\) into a different subspace generated by applying \(A\) on \(U\), and then leads it to an <strong>**</strong>affine<strong>**</strong> space, in an ellipsis centered in \(b\).</p>

<p>In summary, \(\operatorname{lnorm}\) project an embedding vectors \(x \in \mathbb{R}^n\) into a \((n-2)\) dimensional ellipsis, whose shape and location is determined by \(a\) and \(b\).</p>

<h1 id="what-about-rms-layer-normalization">What about RMS Layer Normalization</h1>

<p>This transformation in an ellipsis seems needlessly complicated. What if we instead projected the embedding vectors directly in a sphere, and manipulated that sphere if necessary? This is the intuition of <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>, which sheds the operation \(f\) above, and lets \(b=0\). This way, \(g\) projects the embedding vectors \(x\) into the sphere \(\sqrt{n} S_{n-1}\), and \(a\) just warps the principal axes of this sphere (on the standard basis) to the size \(\sqrt{n} a_i\). That makes interpretation at least way easier. Unfortunately, in the wild, we have mostly layer normalization, so the above interpretation is still the most important one.</p>

  </div><a class="u-url" href="/2023/01/15/visualizing-layer-normalization.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Danilo Naiff&#39;s personal website.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/DFNaiff" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.linkedin.com/in/danilo-naiff" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://scholar.google.com/citations?user=UgDxpKgAAAAJ&hl" target="_blank" title="google_scholar">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#google_scholar"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
