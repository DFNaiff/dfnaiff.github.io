<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-23T17:01:42-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Danilo Naiff</title><subtitle>Danilo Naiff&apos;s personal website.</subtitle><entry><title type="html">A mathematical tutorial on diffusion models.</title><link href="http://localhost:4000/2024/05/28/diffusion-tutorial-1.html" rel="alternate" type="text/html" title="A mathematical tutorial on diffusion models." /><published>2024-05-28T12:00:00-03:00</published><updated>2024-05-28T12:00:00-03:00</updated><id>http://localhost:4000/2024/05/28/diffusion-tutorial-1</id><content type="html" xml:base="http://localhost:4000/2024/05/28/diffusion-tutorial-1.html"><![CDATA[<h1> Preamble </h1>

<h1> Introduction </h1>

<p>We can state the objective of a generative model can be stated, for unconditional generation, as follows:</p>

<blockquote>
  <p>Given some unlabeled data $\mathcal{D} = \{x_1, \ldots, x_D\}$ sampled from a true distribution $q(x)$ <em>unknown</em> to us, how can we, from $\mathcal{D}$, create a distribution $p(x)$ such that:</p>
  <ol>
    <li>The distribution $p(x)$ is close to $q(x)$ in some relevant sense.</li>
    <li>We can easily sample from $p(x)$.</li>
  </ol>
</blockquote>

<p>Notice that we are <em>not</em> looking for any other information from $p(x)$ in principle, except for when it helps us sample from it. In other words, if you are interested in, say, the probability density function of $p(x)$ as a final goal, your problem is different from the one stated above.</p>

<p>In this case of conditional generation, the problem is stated as follows:</p>

<blockquote>
  <p>Given some labeled data $\mathcal{D} = \{(x_1, y_1), \ldots, (x_D, y_D)\}$ sampled from a true distribution $q(x,y) = q(x \mid y) q(y)$ <em>unknown</em> to us, how can we, from $\mathcal{D}$, create a family of distributions distribution $p(x \mid y)$ such that:</p>
  <ol>
    <li>The distribution $p(x \mid y)$ is close to $q(x \mid y)$ in some relevant sense for every $y$.</li>
    <li>We can easily sample from $p(x \mid y)$, given some label $y$.</li>
  </ol>
</blockquote>

<p>Since “for every $y$” may be too ambitious for a goal, we will substitute it instead for</p>
<blockquote>
  <ol>
    <li>The distribution $p(x, y) := p(x \mid y) q(y)$ is close to $q(x, y)$ in some relevant sense.</li>
  </ol>
</blockquote>

<p>Notice that, although we are dealing with labeled data here, the problem is conceptually different than supervised learning. In particular, we are not interested in giving a label $y$ for some given $x$, but instead, <em>given</em> $y$, generating samples $x$ such that $y$ is a correct label for $x$. Of course, conditional generation is reduced to unconditional generation when we are dealing with a single null label $y$.</p>

<p>When dealing with neural networks, our task (when considering the more general case of conditional generation) simplifies to:</p>

<ol>
<li> Creating a distribution $p_\theta(x \mid y)$, parameterized by some set $\theta \in \mathbb{R}^M$ of neural network weights, such that we have an algorithm for sampling from $p_\theta(x \mid y)$ when given $\theta$ and a label $y$. </li>

<li> Creating a differentiable loss function $\mathcal{L}(x, y;\theta)$ such that we are led to $p_\theta(x, y) := p_\theta(x \mid y) q(y)$ being approximately equal to when minimizing

$$
L(\theta) := \mathbb{E}_{x \sim q(x)} L(x; \theta)
$$

</li>

<li> Minimizing $L(\theta)$ by stochastic gradient descent using minibatches of $\mathcal{D}$. </li> </ol>

<p>This is the problem that diffusion models will aim to solve. Diffusion models are not alone in trying to do so, and they are accompanied by techniques such as <a href="https://en.wikipedia.org/wiki/Flow-based_generative_model">Generative Adversarial Networks</a>, <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">Variational Autoencoders</a>, and <a href="https://en.wikipedia.org/wiki/Flow-based_generative_model">Flow-based generative models</a>. It is not the task of this tutorial to explain why diffusion models currently work better than those other models, as I am still confused about this. Therefore, I will instead work on the easier task of just describing diffusion models.</p>

<h1> Sampling through denoising </h1>

<p>For now, let us forget about the full problem. Better, let us forget about learning itself. Consider instead the problem of sampling from some distribution $q(x)$. We will assume that $q(x)$ is a probability distribution in $\mathbb{R}^N$, and devise a method of sampling from $q(x)$ which we will call <em>sampling through denoising</em>.</p>

<p>Even with access to the probability density function $q(x)$, sampling is not a trivial task, as the <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">huge</a> <a href="https://en.wikipedia.org/wiki/Rejection_sampling">amount of</a> <a href="https://en.wikipedia.org/wiki/Importance_sampling">developed</a> <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">sampling</a> <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">techniques</a>, both approximate and exact, can attest. In some senses, our sampling technique will be a particularly ill-suited one, since it will depend on objects whose evaluation will be intractable. However, we will find out that these objects can be <em>learned</em> fairly well by neural networks, and our sampling technique will become very useful in this case.</p>

<h2> Noised distributions </h2>

<p>For devising our technique, we will first need to define the process of <em>noising</em>, in which we create a random variable $X_\sigma$, for $\sigma \geq 0$, through the following process:</p>

<ol>
  <li>Sample $X_0 \sim q(x)$.</li>
  <li>Sample $Z \sim \mathcal{N}(0, I)$, where $\mathcal{N}(0, I)$ is the standard multivariate normal distribution in $\mathbb{R}^N$.</li>
  <li>Let $X_\sigma := X_0 + \sigma Z$.</li>
</ol>

<p>This way, the random variable $X_\sigma$ is distributed according to $p(x, \sigma)$, where $p(x; \sigma)$ is a family of probability distributions whose density is given by</p>

\[p(x; \sigma) := \int_{\mathbb{R}^N} \mathcal{N}(x \mid x_0, \sigma^2 I) q(x_0) dx_0,\]

<p>where $\mathcal{N}(x \mid x_0, \sigma^2 I)$ is the density of the multivariate normal distribution with mean $x_0$ and covariance $\sigma^2 I$, given by</p>

\[\mathcal{N}(x \mid x_0, \sigma^2 I) = \left(2 \pi \sigma \right)^{-N/2} \exp \left( -\frac{1}{2 \sigma^2} \norm{x - x_0}^2 \right).\]

<p>This family of distribution has three fundamental properties. The first two are obvious:</p>

<ol>
  <li>$p(x;0) = q(x)$</li>
  <li>$p(x;\sigma)$ asymptotically converges to $\mathcal{N}(0, \sigma^2 I)$ when $\sigma \to \infty$.</li>
</ol>

<p>The third key property is more involved, in that it satisfies the <em>probability flow ODE</em>, to be defined soon. Before that, we will define the key objects we will be dealing with, which are the <em>noised score functions</em>, defined as</p>

\[s(x, \sigma) := \nabla_x \log p(x;\sigma).\]

<p>As a spoiler, <em>these</em> are the objects that our neural network will be approximating, since they will be the building blocks of our sampling techniques. To see why, we will take a look at the probability flow ODE.</p>

<h2> The probability flow ODE </h2>

<p>Consider the following differential equation, which we will call (surprise) the probability flow ODE</p>

\[\frac{d x}{d \sigma} = -\sigma s(x, \sigma),\]

<p>With $s(x,\sigma)$ defined as above. The probability flow ODE of course defines a deterministic map $x_\sigma = F(x_\tau, \tau, \sigma)$ defined by integrating the above ODE from $\tau$ to $\sigma$, either forward or backward, with initial (or terminal) condition $x_\tau$, resulting in the final value $x_\sigma$.</p>

<p>A major theorem is that this map (under whatever regularity conditions) will have the following property:</p>

<blockquote>
  <p>If $X_{\tau}$ is distributed according to $p(x;\tau)$, then $X_\sigma := F(X_{\tau}; \tau, \sigma)$ is distributed according to $p(x;\sigma)$.</p>
</blockquote>

<p>We will give a rough demonstration of this result below, but before, let us just see why this is an amazing result. Namely, it gives us a general recipe for sampling a random variable $X_\sigma \sim p(x;\sigma)$, given that we know how to sample from $p(x;\tau)$:</p>

<ol>
  <li>Sample $X_\tau$ from $p(x;\tau)$.</li>
  <li>Solve the probability flow ODE from $\tau$ to $\sigma$ with initial (terminal) condition $X_\tau$, using some numerical method.</li>
  <li>The solution $X_\sigma$ is sampled from $p(x;\sigma)$.</li>
</ol>

<p>Why is this a great recipe? Because remember, our original problem is to sample from $q(x) = p(x;0)$. And we <em>know</em> how to (approximately) sample from $p(x;\sigma)$ when $\sigma$ is large, since in this case $p(x;\sigma) \approx \mathcal{N}(0, \sigma^2 I)$, and we know very well how to sample from normal distributions. Therefore, here is a recipe on how to sample from $q(x)$:</p>

<ol>
  <li>Sample $X_{\sigma_\max}$ from $\mathcal{N}(0, \sigma_\max^2 I)$, for $\sigma_\max$ large.</li>
  <li>Solve the probability flow ODE backward from $\sigma_\max$ to $0$, with terminal condition $X_\sigma$, using some numerical method.</li>
  <li>The solution $X_0$ is sampled from $q(x)$.</li>
</ol>

<p>We will call this amazing sampling process <em>denoising</em>. Well… it would be amazing, <em>if</em> we had access to the noised score functions $s(x, \sigma)$. However, remember that</p>

\[s(x, \sigma) := \nabla_x \log p(x;\sigma) = \nabla_x \log \int_{\mathbb{R}^N} \mathcal{N}(x \mid x_0,\sigma^2 I) q(x_0) dx_0.\]

<p>But, we cannot calculate this quantity, because it is a complicated integral depending on $q(x_0)$. Let us face it, calculating this integral is intractable, except if we <em>maybe</em> had some way to sample from $q(x_0)$, which is exactly what we are looking for.</p>

<p>So, why are we studying this method at all? Because of an amazing property of that, although the calculation of the noised score functions $s(x, \sigma)$ are intractable, they can be fairly well <em>approximated</em> by neural networks, through the second key result in which diffusion models depend.</p>

<h2> A sketch of a derivation of the probability flow ODE </h2>

<p>Assume we want to construct a sequence of random variables $X_\sigma$, indexed by $\sigma \geq 0$, with the following properties:</p>

<ul>
<li> $X_\sigma \sim p(x;\sigma)$. </li>
<li> When given some $X_\tau$, $X_\sigma|X_\tau$ is given by evolving _some_ ordinary differential equation

$$
\frac{d x}{d \sigma} = f(x, \sigma),
$$

with initial condition $x(\tau) = X_\tau$.

</li>
</ul>

<p>The second condition above implies that the family of densities $p(x;\sigma)$ should satisfy the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Planck equation</a></p>

\[\pderiv{p(x;\sigma)}{\sigma} = -\nabla_x \cdot \left( f(x, \sigma) p(x;\sigma) \right).\]

<p>It suffices to show then that $f(x, \sigma) = -\sigma \nabla_x \log p(x;\sigma)$ satisfies the equation above, as follows:</p>

<ol>
<li>
First, we notice that, since $p(x;\sigma) = \int_{\mathbb{R}^N} \mathcal{N}(x \mid x_0, \sigma) q(x_0) dx_0$, then the left side of the Fokker-Planck equation becomes

$$
\int_{\mathbb{R}^N} \pderiv{\mathcal{N}(x \mid x_0, \sigma^2 I)}{\sigma} q(x_0) dx_0
$$

</li>
<li>
Plugging in $f(x, \sigma) = \sigma \nabla_x \log p(x;\sigma)$, and noticing that 

$$
p(x;\sigma) \nabla_x \log p(x;\sigma) = \nabla_x p(x;\sigma),
$$
the right side of the Fokker-Planck equation becomes

$$
\nabla_x \cdot \left( -\sigma \nabla_x p(x;\sigma) \right) = \int_{\mathbb{R}^N} \left( -\sigma \nabla^2_x \mathcal{N}(x \mid x_0, \sigma^2 I) \right) q(x_0) dx_0
$$

</li>
<li>
Since the density $\mathcal{N}(x \mid x_0, \sigma^2 I)$ is given by

$$
\mathcal{N}(x \mid x_0, \sigma^2 I) = \left(2 \pi \sigma \right)^{-N/2} \exp \left( -\frac{1}{2 \sigma^2} \norm{x - x_0}^2 \right),
$$

through straightforward (if somewhat laborious) differentiation, we find that

$$
\pderiv{\mathcal{N}(x \mid x_0, \sigma^2 I)}{\sigma} = \sigma \nabla^2_x \mathcal{N}(x \mid x_0, \sigma^2 I),
$$

thus showing that the RHS and the LHS side of the Fokker-Planck equation are equal if $f(x, \sigma) = -\sigma \nabla_x \log p(x;\sigma)$.
</li>
</ol>

<h1> Approximating the noised score functions </h1>

<p>Good. Now, we need to approximate the score function</p>

\[s(x, \sigma) = \nabla_x \log p(x; \sigma) = \nabla_x \log \int_{\mathbb{R}^N} \mathcal{N}(x \mid x_0, \sigma^2 I) q(x_0) dx_0.\]

<p>Let our approximation be a neural network $s_\theta(x, \sigma)$, parameterized by $\theta$. We will first try to do the obvious step: minimize the difference between $s_\theta(x, \sigma)$ and $s(x, \sigma)$, “suitably averaged”. Let us see what we would mean by that.</p>

<p>Remember, we are interested in the score function because we want to solve the probability flow ODE</p>

\[\frac{d x}{d \sigma} = -\sigma s(x, \sigma)\]

<p>backward with $X_{\sigma_\max} \sim \mathcal{N}(0, \sigma_\max^2 I)$ as terminal condition. In this case, we have that, for each $\sigma$, $X_\sigma \sim p(x; \sigma)$. Therefore, it stands to reason that, for each $\sigma$, we want our solution to be accurate where $p(x; \sigma)$ is concentrated.</p>

<p>Thinking as a regression problem, we then want to minimize the difference between $s_\theta(X_\sigma, \sigma)$ and $s(X_\sigma, \sigma)$ with $X_\sigma \sim p(x; \sigma)$. However, we have that $X_\sigma = X + \sigma Z$, with $X \sim q(x)$, $Z \sim \mathcal{N}(0, I)$. For this to work out, we need also to define a distribution $\lambda(\sigma)$ such that $\sigma \sim \lambda(\sigma)$, ideally with the support of $\sigma$ concentrated in $(0, \sigma_{\max})$.</p>

<p>Finally, we need a way of measuring the distance between $s(x, \sigma)$ and $s_\theta(x, \sigma)$. Since they are both vectors in $\mathbb{R}^N$, the natural way of measuring the distance in the squared Euclidean norm $\norm{s_\theta(x, \sigma) - s(x, \sigma)}^2$. Therefore, we arrive at an ideal loss function for our neural network.</p>

\[L^{\text{ideal}}(\theta) = \mathbb{E}_{\sigma \sim \lambda(\sigma)} \mathbb{E}_{X \sim q(x)} \mathbb{E}_{X_\sigma \sim \mathcal{N}(X, \sigma^2 I)} \norm{s_\theta(x, \sigma) - s(x, \sigma)}^2.\]

<p>The obvious problem here is that we cannot compute $L^{\text{ideal}}(\theta)$, since we cannot compute $s(x, \sigma)$. However, the second main theorem of diffusion models will come to help us, saying that minimizing $L^{\text{ideal}}(\theta)$ is equivalent to minimizing a much easier loss function.</p>

<h2> Score matching </h2>]]></content><author><name></name></author><summary type="html"><![CDATA[Preamble]]></summary></entry><entry><title type="html">Entropy and the world - Part 3: Interlude on QM.</title><link href="http://localhost:4000/2024/04/10/thoughts-on-entropy-3.html" rel="alternate" type="text/html" title="Entropy and the world - Part 3: Interlude on QM." /><published>2024-04-10T12:00:00-03:00</published><updated>2024-04-10T12:00:00-03:00</updated><id>http://localhost:4000/2024/04/10/thoughts-on-entropy-3</id><content type="html" xml:base="http://localhost:4000/2024/04/10/thoughts-on-entropy-3.html"><![CDATA[<h1 id="preamble">Preamble</h1>

<p>Previously, we justified the maximum entropy principle being an actual physical principle for “most physics”, when we consider coarse measurements in time and space. However, “most physics” is not enough. After all, in the previous discussion, we had to assume a dynamical law with fluctuations. This is an approximation of the actual world, and, to get deeply into how entropy works in this world here, we will need to interact with the actual physics of the world.</p>

<h1 id="the-quantum-mechanical-approach-states-and-measurement">The quantum mechanical approach: states and measurement.</h1>

<p>To continue our exploration of entropy in physical systems, we will have to move to quantum mechanics. Although this may seem like it makes the problem much harder in principle, we will find that the quantum mechanical framework not only is appropriate because it is true but because it ends up making things much easier.</p>

<p>As a primer, we review the <a href="https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics#Postulates_of_quantum_mechanics">basic framework</a> of quantum mechanics, for a single isolated physical system. First, we have to define what are the systems we are describing. In quantum mechanics, we have that each isolated system is associated with a separable Hilbert space $\mathbb{H}$ with inner product $\braket{\phi}{\psi}$, or, in a very informal language, to a vector space $\mathbb{C}^D$ with $D$ being possibly <em>very</em> large, that is, infinite. At each time $t$, the physical system is at a state $\ket{\psi} \in \mathbb{H}$. We assume those states to be unit vectors, that is, $\braket{\psi}{\psi} = 1$.</p>

<p>Physical systems are to be measured. In quantum mechanics, we assume that each measurable physical quantity is associated with a Hermitian operator $A$. To avoid using the full apparatus of <a href="https://en.wikipedia.org/wiki/Spectral_theorem">spectral theory</a>, of which I only know the basics, we treat $A$ as we would in the finite-dimensional case (in other words, we assume $A$ to be a <a href="https://en.wikipedia.org/wiki/Compact_operator_on_Hilbert_space">compact Hermitian operator</a>). Therefore, we can use the spectral theorem to postulate that $A$ can be uniquely decomposed as</p>

\[A = \sum_i \lambda_i P_i,\]

<p>where $\lambda_1, \lambda_2, \ldots$ are distinct eigenvalues of $A$, and $P_1, P_2, \ldots$ are projections onto a set of orthogonal subspaces spanning $A$. We abuse our notation and also denote these spaces as $P_1, P_2, \ldots$. The reason why we are directly talking about projections, instead of eigenvalues, is that the fact that the spectra can be degenerated is crucial in what is to follow.</p>

<p>With the above decomposition, we can describe the measurement of a system as follows. When we make a measurement $A$ on the physical system in state $\ket{\psi}$, two things happens:</p>

<ol>
  <li>The result of our measurement is given by one of the eigenvalues $\lambda_1, \lambda_2, \ldots$, with the probability of measuring each eigenvalue $\lambda_i$ being given by $p_i = \bra{\psi} P_i \ket{\psi}$.</li>
  <li>Having the measurement result $\lambda_i$, our system immediately transitions (collapses) from state $\ket{\psi}$ to the normalized state $\frac{P_i \ket{\psi}}{\bra{\psi} P_i \ket{\psi}}$. That is, our system is projected to the subspace $S_i$ and then normalized.</li>
</ol>

<p>The above axiom gives rise to the (main problem)[htteigenbasisps://en.wikipedia.org/wiki/Measurement_problem] of (interpreting quantum mechanics)[https://en.wikipedia.org/wiki/Interpretations_of_quantum_mechanics], but, for now, we will remain agnostic on what measurement <em>really</em> means, and just accept that this is how the world works.</p>

<p>Importantly, we can calculate the expected value $\mean{A}$ of our measurement $A$ (of the physical system in state $\ket{\psi}$) as</p>

\[\mean{A} = \sum_i \lambda_i \bra{\psi} P_i \ket{\psi} = \bra{\psi} \sum_i \lambda_i P_i \ket{\psi} = \bra{\psi} A \ket{\psi}.\]

<p>Also, the measurement axiom makes clear why we want state vectors to be normalized, given that we want our probabilities to sum to one</p>

\[1 = \sum_i p_i = \sum_i \bra{\psi} P_i \ket{\psi} = \bra{\psi} \sum_i P_i \ket{\psi}.\]

<p>Also, the projection themselves $P_i$ can be thought of as measurable physical quantities, since they are Hermitian operators. In particular, we can think of $P_i$ as answering the question “Has the system collapsed to the subspace $S_i$?” with the value of 1 if the answer is “yes”, and 0 if the answer is no. The expected value here is exactly $\bra{\psi} P_i\ket{\psi}$, which measures “how much” was state vector $\ket{\psi}$ overlapping with subspace $S_i$ to begin with.</p>

<p>A particular projection we have is the projection $P_\psi := \ket{\psi} \bra{\psi}$, associated with the quantum state $\ket{\phi}$. To indicate we are talking about a function of the physical state, rather than a measurement, we refer to this projection as the density operator and use the symbol $\rho$ instead of $P_\psi$. Three properties of the density operator, that can be easily proven, are</p>

\[\trace{\rho} = 1 \\
\bra{\psi} A \ket{\psi} = \trace{\rho A} \\
\frac{P_i \ket{\psi}}{\bra{\psi} A \ket{\psi}} \frac{\bra{\psi} A}{\bra{\psi} A \ket{\psi}} = \frac{A \rho A}{\trace{A \rho A}}.\]

<p>Thus, we can use the density operator $\rho$ in place of the state vector $\ket{\psi}$ when defining our physical system. An advantage of the density operator is that the state vectors $\ket{\psi}$ and $e^{i \theta} \ket{\psi}$ describe the same physical system, so we do not have a unique representation. However, since we have that $\ket{e^{i \theta} \ket{\psi}} \bra{e^{i \theta} \ket{\psi}} = \ket{\psi} \bra{\psi}$, we do have a unique representation of our physical system state when given by the density matrix $\rho$.</p>

<h1 id="mixed-systems-and-the-density-matrix">Mixed systems and the density matrix.</h1>

<p>First, consider a pure system as above, in state $\ket{\psi}$. associated with this quantum state, we have the projection operator $P_{\psi}:= \ket{\psi} \bra{\psi}$. To indicate we are talking about a function of the physical state, rather than a measurement, we refer to this projection as the density operator and use the symbol $\rho$ instead of $P_\psi$. Three properties of the density operator, that can be easily proven, are</p>

\[\trace{\rho} = 1 \\
\bra{\psi} A \ket{\psi} = \trace{\rho A} \\
\frac{P_i \ket{\psi}}{\bra{\psi} A \ket{\psi}} \frac{\bra{\psi} A}{\bra{\psi} A \ket{\psi}} = \frac{A \rho A}{\trace{A \rho A}}.\]

<p>Thus, we can use the density operator $\rho$ in place of the state vector $\ket{\psi}$ when defining our physical system. An advantage of the density operator is that the state vectors $\ket{\psi}$ and $e^{i \theta} \ket{\psi}$ describe the same physical system, so we do not have a unique representation. However, since we have that $\ket{e^{i \theta} \ket{\psi}} \bra{e^{i \theta} \ket{\psi}} = \ket{\psi} \bra{\psi}$, we do have a unique representation of our physical system state when given by the density matrix $\rho$.</p>

<p>Now, suppose that we do not know in which quantum state our system is in. That is, our system can be in many possible orthogonal states ${\ket{\psi_j}}$, with probabilities ${w_j}$, such that $\sum_j w_j = 1$. Those are associated with pure density matrices ${\ket{\psi_j} \bra{\psi_j}}$. Now, when making a measurement $A$ of this system, we need to consider not only the probabilities given by Born’s rule but also the ones given by our lack of knowledge. We define then the density matrix $\rho$, encoding the state of our system <em>given the uncertainty</em> as</p>

\[\rho := \sum_j w_j \rho_j, \quad \rho_j := \ket{\psi_j} \bra{\psi_j}.\]

<p>The nice thing about the density matrix is that we can easily extend our postulates when written in terms of pure density matrices $\rho_j$. To see this, denote the random variable <em>our system is in quantum state $\ket{\psi_j}$ as $\Psi_j$, and, for a measurement $A$, denote the random variable _result of measurement $A$</em> as $\lambda(A)$. We then can find many similar results. For instance, the expected value of $\lambda(A)$ is given by</p>

\[\trace{\rho A}\]

<p>In particular, writing $A = \sum_i \lambda_i P_i$, the probability $P(\lambda(A) = \lambda_i)$ that we receive measurement $\lambda_i$ is given by</p>

\[P(\lambda(A) = \lambda_i) = \sum_j P(\lambda(A) = \lambda_i \mid \Psi = \psi_j) P(\Psi = \psi_j) = \sum_i \trace{\rho_i P_i} w_i = \trace{\rho P_i}.\]

<p>Similarly, the expected value $\mean{A}$ of $\lambda(A)$ equals</p>

\[\mathbb{E} [\lambda(A)] = \trace{\rho A}.\]

<p>After measurement, $\lambda(A) = \lambda_i$, the quantum state of the system will be in some unknown new collapsed state $P_i \rho_j P_i / \trace{\rho_j P_i}$ with some posterior probability $P(\Psi = \psi_j \mid \lambda(A) = \lambda_i)$ that I <em>was</em> in state $\ket{\psi_j}$ given that I’ve observed $\lambda_i$. This can also be described by a density matrix $\rho, and using Bayes’ rule, we find that the density matrix collapses to</p>

\[\frac{P_i \rho P_i}{\trace{\rho P_i}}.\]

<p>Finally, the matrix $\rho$ also has unit trace</p>

\[\trace{\rho}.\]

<p>Moreover, we have the property that</p>

\[\trace{\rho^2} \leq 1,\]

<p>with equality only if $\rho = \ket{\psi} \bra{\psi}$, that is, our system is in a pure state.</p>

<p>In what follows, we will want to write the density matrix given a set of orthogonal subspaces, whose projections are given by ${Q_j}$. We will often also refer to the subspace themselves as $Q_j$. The probability $w_j$ will now mean “the probability that our system’s quantum state is in subspace $Q_j$”, and we will write</p>

\[\rho = \sum w_j Q_j,\]

<p>where now $w_1, w_2, \ldots$ are <em>distinct</em> positive values satisfying</p>

\[\trace{\rho} = \sum_j w_j n_j = 1.\]

<p>This way we are given a unique representation of the density matrix $\rho$ given ${w_j}$ and ${Q_j}$. If, for each subspace ${Q_j}$, of dimension $n_j$, we choose a basis ${\ket{\psi}_{jk}}_{k=1}^{n_j}$, we can diagonalize $\rho$ as</p>

\[\rho = \sum_j \sum_{j=1}^{n_j} w_j \ket{\psi}_{jk} \bra{\psi}_{jk} \quad n_j := \operatorname{rank}{Q_j}.\]

<p>In this case, we are tempted again to interpret $\rho$ as saying that “the system is in state $\ket{\psi}_{jk}$ with probability $w_j$”. The problem here is that the orthogonal basis of $Q_j$ is not unique, so in a sense saying the previous sentence is arbitrary. What we can say instead is just “the system’s state is in subspace $Q_j$ with probability $w_j n_j$”, but we can say nothing about any specific state inside $n_j$.</p>

<p>With these properties, we can consider density matrices as the fundamental objects describing our physical system, instead of quantum states. Following <a href="https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176">Nielsen and Chuang</a>, we reformulate the state and measurement postulates in terms of density matrices:</p>

<p><em>Postulate 1</em>: Associated to any isolated physical system is a complex vector space $\mathbb{H}$ with inner product $\braket{\phi}{\psi}$. The system is described by its density operator $\rho$, which is both positive-definite and has unit trace. This implies that the density matrix can be decomposed as</p>

\[\rho = \sum_j w_j \trace{Q_j},\]

<p>where ${Q_j}$ are projections into a set of orthogonal subspaces, and $w_j &gt; 0$ are distinct values. Notice that the exigence of $\trace{\rho} = 1$ implies that</p>

<p>The system is in a <em>pure state</em> if $\rho = \ket{\psi} \bra{\psi}$, and in a <em>mixed state</em> otherwise. Equivalently, a system is in a pure state if $\trace{\rho^2} = 1$ and in a mixed state if $\trace{\rho^2} &lt; 1$. If a quantum system is in state $\rho_k$ with probability $p_k$, then its density operator is given by</p>

\[\rho = \sum_j p_k \rho_k.\]

<p><em>Postulate 2</em>: Quantum measurements are described by Hermitian operators $A$, which can be decomposed as</p>

\[A = \sum_i \lambda_i P_i,\]

<p>where ${P_i}$ are projections into a set of orthogonal subspaces that span $\mathbb{H}$, and ${\lambda_i}$ are the distinct possible measurement results. If the state of the system before measurement, is $\rho$, the probability that we measure value $\lambda_i$ is given by</p>

\[p(\lambda_i) := \trace{\rho P_i},\]

<p>and, measuring value $\lambda_i$, the system state collapses to a new state $\rho(P_i)$, given by</p>

\[\rho(P_i) := \frac{P_i \rho P_i}{\trace{\rho P_i}}.\]

<p>In particular, this implies that the expected value $\mean{A}$ of the measurement $A$ is given by</p>

\[\mean{A} = \trace{\rho A}.\]

<h1 id="composite-systems-and-entanglement">Composite systems and entanglement</h1>

<p>Another axiom concerns composite physical systems made of more than one part. In terms of state vectors, the third postulate of quantum mechanics says that, if we we have two distinct physical systems with associated Hilbert spaces $\mathbb{H}_A$ and $\mathbb{H}_B$, then the resulting composite physical system Hilbert space $\mathbb{H}$ is the tensor product</p>

\[\mathbb{H} = \mathbb{H}_A \otimes \mathbb{H}_B\]

<p>Moreover, <em>if</em> the the systems are prepared in states $\ket{\psi_A}$ and $\ket{\psi_B}$, then the composite system state vector $\ket{\psi}$ is</p>

\[\ket{\psi} = \ket{\psi_A} \otimes \ket{\psi_B}.\]

<p>we formulate the composite system postulate in terms of the density operator, by giving <em>Postulate 3</em>: Assuming we have $N$ distinct physical systems, each associated with a Hilbert space $\mathbb{H}_n$, then the resulting composite physical system Hilbert space $\mathbb{H}$ is the tensor product $\mathbb{H}_1 \otimes \ldots \otimes \mathbb{H}_N$. If each system $i$ is prepared in state $\rho_i$, then the joint state $\rho$ of the composite system is given by $\rho_1 \otimes \ldots \otimes \rho_N$.</p>

<p>By choosing an orthogonal basis ${\ket{a}_i}$ for $\mathbb{H}_A$ and another orthogonal basis ${\ket{b}_j}$ for $\mathbb{H}_B$, we have that ${\ket{a}_i \otimes \ket{b}_j}$ is a basis for $\mathbb{H}_A \otimes \mathbb{H}_B$, so we can write $\ket{\psi}$ as</p>

\[\ket{\psi} = \sum_{i,j} c_{ij} \ket{a_i} \otimes \ket{b}_j.\]

<p>Similarly, we can write the density operator $\rho = \ket{\psi} \bra{\psi}$ as</p>

\[\rho = \sum_{i,j,k,l} c_{i,j} c_{k, l}^* \ket{a_i} \bra{a_k} \otimes \ket{b_j} \bra{b_l}.\]

<p>Crucially, not all state vectors $\ket{\psi} \in \mathbb{H}$ can be written as $\ket{\psi_A} \otimes \ket{\psi_B}$. In this case, we will not be able to consider each of the subsystems as being in some state vector. This gives us another strong motivation for dealing with the density operator formulation of quantum mechanics because if the system is in state $\rho = \ket{\psi}\bra{\psi}$, we <em>can</em> consider each system as in states $\rho_A$ and $\rho_B$, given by</p>

\[\rho_A = \ptrace{B}{\rho} \\
\rho_B = \ptrace{A}{\rho},\]

<p>where we define the partial trace as</p>

\[\ptrace{B}{\rho} = \left(\sum_n I_A \odot \bra{b_n}\right) \rho \left(I_A \odot \ket{b_n}\right) \\
= \sum_n c_{in} c_{kn} \ket{a_i} \bra{a_n}.\]

<p>This is because, using some calculations, we can show that, for any measurement $M_A$ made in the first system, the corresponding measurement in $\mathbb{H}_A \otimes \mathbb{H}_B$ is $M_A \otimes I_B$, and that</p>

\[\mean{M_A} = \trace{\rho \left(M_A \otimes I_B\right)} = \trace{\ptrace{B}{\rho} M_A},\]

<p>We will have much more to say about entanglement in the future, as I strongly suspect that this will be key to understanding the maximum entropy principle. However, that is a future discussion.</p>

<h1 id="time-evolution">Time evolution</h1>

<p>Finally, the last axiom concerns the time evolution of a physical system. In particular, it postulates that, when not perturbed by some measurement, the state of our system $\ket{\psi_0}$ at time $0$ transitions to state $\ket{\psi_t}$ at time $t$ as</p>

\[\ket{\psi_t} = U(t) \ket{\psi_0},\]

<p>where the operator $U(t)$ is unitary, with $U(0) = I$. The fact that $U(t)$ is unitary satisfies the requirement that $\braket{\psi}{\psi}$ is always equal to one.</p>

<p>Since the operator is unitary  we can show that</p>

\[U'(t) = -\frac{i}{\hbar} H,\]

<p>where $H$ is a Hermitian operator. The operator $H$ is called the Hamiltonian, which, when seen as a measurement, measures the total energy of the system. Therefore, the system will evolve according to the Schrödinger equation
\(\pderiv{\ket{\psi}}{t} = -\frac{i}{\hbar} H \ket{\psi}.\)</p>

<p>The time evolution postulate and the Schrödinger equation are equivalent, and we can begin with the Schrödinger equation as our postulate, and derive unitary evolution by solving it.</p>

<p>In terms of density operators, we can formulate <em>Postulate 4</em>, which says that, when not perturbed by some measurement, the state of our system $\rho_0$ at time $0$ transitions to state $\rho_t$ at time $t$ as</p>

\[\ket{\psi_t} = U(t) \rho U(t)\]

<p>where the operator $U(t)$ is unitary, with $U(0) = I$. The Schrödinger equation for the density matrix can be written as</p>

\[\pderiv{\rho}{t} = -\frac{i}{\hbar} \left[H, \rho\right] \\
\left[H, \rho\right] = H\rho - \rho H\]

<p>For now, we will consider our system as static in time, bar a measurement, and we will not consider time evolution until much later in our essays. Instead, in the next part, we will use the above formalism to consider the maximum entropy principle in operator form, and its implications.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Preamble]]></summary></entry><entry><title type="html">Entropy and the world - Part 2</title><link href="http://localhost:4000/2024/03/18/thoughts-on-entropy-2.html" rel="alternate" type="text/html" title="Entropy and the world - Part 2" /><published>2024-03-18T12:00:00-03:00</published><updated>2024-03-18T12:00:00-03:00</updated><id>http://localhost:4000/2024/03/18/thoughts-on-entropy-2</id><content type="html" xml:base="http://localhost:4000/2024/03/18/thoughts-on-entropy-2.html"><![CDATA[<p><a href="/2024/03/13/thoughts-on-entropy-1.html">Part 1</a></p>

<h1 id="preamble">Preamble</h1>

<p>So, in the previous post, we concluded that, if we assume the maximum entropy principle, we arrive at classical thermodynamics. This is the basis of statistical mechanics, and nothing new was said there. Yet, a question remains: why? Why does any of this work? In this part, we will make some heuristic arguments for why this works, which will be arguments sort of agnostic on which physical system we are considering. However, we will get to the actual physics of the actual world in the next part.</p>

<h1 id="microstates-and-macrostates-intuitively">Microstates and macrostates, intuitively.</h1>

<p>In the previous post, we considered a finite number of states $\theta_1, \ldots, \theta_m$, to which we applied the maximum entropy principle. However, here, we will have to be more careful. In particular, if we are to derive the maximum entropy principle, we will need to consider two different kinds of states in our problem. Also, notice we are now assuming a finite set of states $m$, in contrast to the previous post. This is intentional, and we do so because most of our arguments are going to be counting arguments.</p>

<p>Before moving on to formal definitions, we explain microstates and macrostates intuitively. Imagine a system of $N$ particles with unit mass, labeled each as $i=1, \ldots, N$. Assume that these particles are moving in a three-dimensional space (for instance, a closed box), following standard Newtonian mechanics. The individual state of each particle $i$ are its position $\mathbf{x}_i$ and velocity $\mathbf{v}_i$, so that its state is given by $\theta_i = (\pmb{x}_i, \pmb{v}_i) \in \mathbb{R}^6$. Our microstate, which in Newtonian mechanics we would call just state, is then the position and velocity of each of these $N$ particles, or, in other words, our microstates $\xi$ are vectors $\theta^{(N)} \in \mathbb{R}^{6N}$.</p>

<p>Now, the truth is, we don’t care about the state of each of these particles. We only care about questions like “How many of these particles have kinetic energy above some value $u_0$” or “How many of these particles can I find at distance $r$ from position $x_0$?”. Equivalently, we care about the probability (density) $p(\theta) \in \mathcal{P}(\theta)$ that I will find some particle at position $(\pmb{x_i}, \pmb{v}_i)$. We will call such probability density $q \in \mathcal{P}(\mathbb{R}^6)$. We will call this probability the macrostate of our system.</p>

<p>Notice that the important bit here is that the dimensionality of the random variable we care about is just $\mathbb{R}^{6}$, which is much less than $\mathbb{R}^{6N}$. Sure, care in fact about the space of probability distributions $p \in \mathcal{P}(\mathbb{R}^6)$, which lives in another type of space, so such comparisons cannot be done so easily. However, assume we divided $\mathbb{R}^{6}$ in a set of $M$ boxes, labeled by $m=1, \ldots, M$, such that our individual states $\theta_i$ are given by “$(\pmb{x}_i, \pmb{v}_i)$ is in box $m$”. In other words, our individual state space is now a finite set $[M] = {1, \ldots, M}$, and our microstate space is $[M]^N$, whose size is given by $M^N$. Crucially, the possible probabilities can only be given by</p>

<p>\(\{(q_1 = n_1/N, \ldots, q_m = n_m/N); n_j \geq 0, n_1 + \ldots + n_M = N\}\).</p>

<p>Therefore, the possible probabilities are isomorphic to the set of <a href="https://en.wikipedia.org/wiki/Composition_(combinatorics)">compositions</a> of $N$ in $M$ pieces (including 0). That is, each element of this set consists of a set $\pmb{n}(n_1, \ldots, n_m)$ whose value $n_j$ means “number of microstates in individual state $j$”. Naming this set as</p>

\[C[N, M] := \{\pmb{n} = (n_1, \ldots, n_m) ; n_j \geq 0, n_1 + \ldots + n_M = N\},\]

<p>we find through some simple combinatorics that the size of this set is given by</p>

\[\abs{C[N, M]} = \frac{(N + M - 1)!}{N!(M-1)!},\]

<p>which is smaller than $M^N$. Therefore, <em>many microstates will correspond to the same microstate</em>. This is the crucial setting in which we will make our argument.</p>

<p>Just to formalize better, we will then consider a set $\Theta$ of individual states, whose elements are $\theta$, and, for $N$ copies of the state, we will consider the microstate set as $\Theta^N$, whose elements are given by $\theta^{(N)}$. Finally, the macrostate set will be $\mathcal{P}(\Theta)$, with an element $q \in \mathcal{P}(\Theta)$ being the macrostate of our system. If we consider $\Theta = [M]$, we will then consider also $C[N, M]$ as described above, which, for simplicity, we will also call the macrostate of our system, with each macrostate being a set $\pmb{n} = (n_1, \ldots, n_M) \in C[N, M]$.</p>

<h1 id="a-counting-argument">A counting argument.</h1>

<p>With that in mind, we can imagine that we have $N$ copies of our physical system, each labeled $i=1, \ldots, N$, such that each of these systems can be in one of $M$ possible states, labeled $j=1, \ldots, M$. Each individual state is given by $\theta_i \in [M]$, and we let the microstate be $\pmb{\theta} \in [M]^N$. Therefore, we can ask for each macrostate $\pmb{n} \in C[N, M]$ the following question:</p>

<blockquote>
  <p>How many possible set of macrostates $\pmb{\theta}$ of each individual copy are compatible with a fixed $\pmb{n}$?</p>
</blockquote>

<p>We refer to this amount as $W(\pmb{n}; N)$. Again, simple combinatorics give us the answer as</p>

\[W(\pmb{n};N) = \frac{N!}{n_1! \ldots n_M!}.\]

<p>Now, if we assume $N$ to be <em>very</em> large, each $\pmb{n}$ with $W(\pmb{n}; N)$ will also have $n_j$ large for every $j$. Therefore, defining $S_I(\mathbf{n};N) := \frac{1}{N} \log W(\pmb{n};N)$, we can deploy Stirling’s approximation to find that</p>

\[S\_I(\mathbf{n};N) := \frac{1}{N} \log W(\pmb{n};N) = - \sum_{j} \frac{n_j}{N} \log \frac{n_j}{N}.\]

<p>Therefore, we find that $S_I(\mathbf{n};N)$ is the information entropy of the distribution ${q_j = n_j/N}_{j=1}^M$. So, we find that, when maximizing the information entropy for ${q_j}$, for $N$ copies of the physical system, we find the macrostate $\pmb{n} = (n_1, \ldots, n_j)$ that is realizable by most microstates. Moreover, for large $N$, this is overwhelmingly larger, because, for any other $\mathbf{n}’$ with $S_I(\mathbf{n}’;N) &lt; S_I(\mathbf{n}’;N)$, we find that</p>

\[\frac{W(\pmb{n}';N)}{W(\pmb{n};N)} \approx e^{-N \left(S\_I(\mathbf{n};N) - S\_I(\mathbf{n}';N)\right)}\]

<p>Therefore, one tentative answer to the question “Why is the maximum entropy principle valid?” can be as follows:</p>

<blockquote>
  <p>In general, our system is jumping very rapidly between individual states, so that in practice we are always dealing with an ensemble of $N$ individual states. Maximizing the maximum information entropy is equivalent to finding the macrostate that can be realized by most microstates.</p>
</blockquote>

<p>This is the answer by Boltzmann and Gibbs, and through this answer, statistical mechanics can be developed and turned into a rich framework. Yet, although the answer works in practice, it does not work in theory, or at least is not complete, as we will see below.</p>

<h1 id="a-probabilistic-counterargument">A probabilistic counterargument.</h1>

<p>The problem here lies in the “jumping very rapidly between individual states” part. Let us be more clear about what is happening here.</p>

<p>Assume that our physical system jumps between individual states at discrete times $t=0, \delta, 2 \delta, \ldots$, such that $\delta$ is a characteristic jumping time. At each time $t$, our system has a latent state $\xi_t$ of a possible set of latent states $\Xi$, such that both:</p>

<ul>
  <li>Our transition dynamic is Markovian in the latent space, so that</li>
</ul>

\[\xi_{t+\delta} \sim p_\xi(\xi_{st+\delta} \mid \xi_t),\]

<ul>
  <li>And our individual states $\theta_i$ depends only on the latent state $\xi_t$, such that</li>
</ul>

\[\theta_t \sim p_{\theta|\xi}(\theta_t|\xi_t).\]

<p>Now, for the sake of argument, we assume that our system dynamics reaches a stationary distribution $p_{\xi}(x_t)$, inducing a stationary distribution on the individual state $p_\theta(\theta_t)$. Moreover, we assume that we sample our system in long enough intervals $T$ such that $\theta_{t+T}$ is essentially independent of $\theta_t$ (this is just to avoid details involving hidden Markov chains). Therefore, we can assume that, when sampling $N$ individual states in such way, for each $t=1, \ldots, N$, we have that</p>

\[\theta_t \sim p(\theta); \quad \theta^N_t \sim \prod p(\theta_i) \\
p(\theta=j) = p_j, \quad j=1, \ldots, M.\]

<p>Therefore, we find that the probability of each macrostate $p(\pmb{n}) = p(n_1, \ldots, n_M)$ is given the multinomial distribution</p>

\[p(\pmb{n}) = p(n_1, \ldots, n_M) = W(\pmb{n};N) \exp( \sum_j n_j \log p_j).\]

<p>By applying again the Stirling’s approximation, we find that</p>

\[\log p(\pmb{n}) = \sum_{j=1}^M \frac{n_j}{N} \left(\log \frac{n_j}{N} - \log p_j\right).\]

<p>Maximizing $\log p(\pmb{n})$ let us find that</p>

\[n_j \approx N p_j, \quad j=1, \ldots, M\]

<p>Here we find a problem. In general, ${N p_j}$ is <em>not</em> the macrostate with the maximum possible amount of microstates, unless we postulate that $p_i$ is the maximum information entropy distribution. But we fall in a loop because what we wanted to do was to justify this postulate <em>in the first place</em>! Therefore, following Boltzmann’s argument to its logical conclusion leaves us with circular reasoning. So, although our problem is solved in practice, we still do not have an understanding of why the maximum entropy principle is thus in the actual world. This a problem. In the next part, I’ll follow Grandy’s approach and get into the depths of quantum mechanics to try to get a glimpse into the problem.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 1]]></summary></entry><entry><title type="html">Entropy and the world - Part 1</title><link href="http://localhost:4000/2024/03/13/thoughts-on-entropy-1.html" rel="alternate" type="text/html" title="Entropy and the world - Part 1" /><published>2024-03-13T12:00:00-03:00</published><updated>2024-03-13T12:00:00-03:00</updated><id>http://localhost:4000/2024/03/13/thoughts-on-entropy-1</id><content type="html" xml:base="http://localhost:4000/2024/03/13/thoughts-on-entropy-1.html"><![CDATA[<h1 id="updates">Updates</h1>

<p>2024-04-10 - Corrected some clumsy notation.</p>

<p>2024-03-15 - Understood a bit more of the theory, in particular the relation with the volume. The text reflects that chance. Also, the first version of this text had the final part incomplete, reflecting that lack of previous understanding.</p>

<h1 id="preamble">Preamble</h1>

<p>One thing that has been on my mind, on and off, for a long time, is thermodynamics. It may be that, for a while, thermodynamics just seemed <em>mysterious</em>, with a bunch of heuristics that did not make much sense, and lots of talks about engines, cycles and so on. Recently, I delved more deeply into the axiomatic approach given by <a href="https://www.amazon.com/Thermodynamics-Intro-Thermostat-2E-Clo/dp/0471862568">Callen’s book</a>, and suddenly classical thermodynamics made sense to me. Sure, entropy was still a mysterious object, but it was a <em>well-behaved</em> mysterious object, following certain laws, just like force or mass. The laws of thermodynamics made sense, and we had a complete theory. I could grasp classical thermodynamics.</p>

<p>Still, there was the classical and statistical thermodynamics correspondence. I was now curious about how this correspondence worked and if I could get by myself some nice insight. Also, there seems to be a deeper topic here. The connection between the microscopic world and the macroscopic world passes through statistical mechanics, it seems. So, in a poorly formed way, part of the mystery of how the world is what it is seems to pass through that connection.</p>

<p>After some research and some thoughts, inspired mainly by <a href="https://www.amazon.com/Entropy-Evolution-Macroscopic-International-Monographs/dp/0199546177/">Walter Grandy’s book</a>, and some Wikipedia pages, I could at least form some inner thoughts on that. And, well, entropy did not become <em>less</em> mysterious, but it is now mysterious in a different way. In a sense, classical thermodynamics is just the maximum entropy principle, a mathematical subject, and not only a physical subject <em>per se</em>. I got excited, and decided to give my thoughts on the subject, because why not? That is why I have a website, after all.</p>

<h1 id="classical-thermodynamics">Classical thermodynamics</h1>

<p>In Callen’s, classical thermodynamics is formulated through a set of postulates, for macroscopic systems. They are as follows:</p>

<blockquote>
  <p><em>Postulate I</em>. There exist particular states (called equilibrium states) of
systems that, macroscopically, are characterized completely by the
internal energy $U$, and a set of extensive parameters $X_1, \ldots, X_n$ to be later specifically enumerated.</p>
</blockquote>

<p>Which of these extensive parameters is to be used depends on the nature of our system. In practice, we’ll focus on simple systems, defined as</p>

<blockquote>
  <p>Systems that are macroscopically homogeneous, isotropic, and uncharged,
that are large enough so that surface effects can be neglected, and that are
not acted on by electric, magnetic, or gravitational fields.</p>
</blockquote>

<p>In simple systems, we can consider as extensive parameters only the volume $V$ and the number of specific species $N_1, \ldots, N_r$ constituents of our system. Philosophically, the most important thing about this postulate is that there <em>is</em> an internal energy variable $U$ at all, which is not obvious at all. The second and third postulates will introduce our second key player in thermodynamics, entropy.</p>

<blockquote>
  <p><em>Postulate II</em>. There exists a function (called the entropy) of the extensive
parameters, defined for all equilibrium states, and having the following
property. The values assumed by the extensive parameters in the absence of a
constraint are those that maximize the entropy over the manifold of con-
strained equilibrium states.</p>
</blockquote>

<blockquote>
  <p><em>Postulate III</em>. The entropy of a composite system is additive over the
constituent subsystems (whence the entropy of each constituent system 1s a
homogeneous first-order function of the extensive parameters). The entropy is
continuous and differentiable and is a monotonically increasing function of
the internal energy.</p>
</blockquote>

<p>Notice that these postulates are not in the game of explaining what entropy <em>is</em>, just that there is something called entropy that is maximized, and has the properties described in the third postulate. Therefore, classical thermodynamics is a phenomenological theory, that has no business in interpreting the phenomena it describes, but just describing the actual phenomena.</p>

<p>Focusing on simple systems, we denote the entropy function as</p>

\[S = \hat{S}(U, V, N_1, \ldots, N_r),\]

<p>and the second postulate implies that this function is always to be maximized, under certain constraints. In particular, if we assume that the universe evolves by the removal of constraints specified by its initial conditions, this postulate <em>is</em> the second law of thermodynamics. The third postulate implies some properties of the entropy. A fundamental one is that we can invert $\hat{S}$ with relation to $U$, therefore arriving at the energy formulation</p>

\[U = \hat{U}(S, V, N_1, \ldots, N_r).\]

<p>By taking the differential of $U$, we arrive at</p>

\[dU = \frac{\partial{\hat{U}}}{\partial S} dS + \frac{\partial{\hat{U}}}{\partial V} dV + \sum_{j=1}^r \frac{\partial{\hat{U}}}{\partial N_j} dN_j.\]

<p>Now, we <em>define</em> the temperature $T$, pressure $P$ and chemical potentials $\{\mu_j\}$ as</p>

\[T := \frac{\partial{\hat{U}}}{\partial S} \\
P := -\frac{\partial{\hat{U}}}{\partial V} \\
\mu_j := -\frac{\partial{\hat{U}}}{\partial N_j},\]

<p>thus arriving at the first law of thermodynamics (where $dQ = T dS$ and $dW = P dV$)</p>

\[dU = T dS - P dV + \sum_{j=1}^r \mu dN_j\]

<p>Of course, we must show that our definition of temperature and pressure corresponds to what we know to be temperature and pressure, but that <em>can</em> be shown (just check the first few chapters of <em>Callen</em>!). Given these definitions in terms of $\hat{U}$, we can define the same variables in term of the $\hat{S}$ (using some manipulations of partial derivatives) as</p>

\[\frac{1}{T} = \frac{\partial \hat{S}}{\partial U} \\
\frac{P}{T} = \frac{\partial \hat{S}}{\partial V} \\
\frac{\mu_j}{T} = -\frac{\partial \hat{S}}{\partial N_j}\]

<p>and write the differential of $S$ as</p>

\[dS = \frac{1}{T} dU + \frac{P}{T} dV \sum_{j=1}^r \frac{\mu_j}{T} d N_j.\]

<p>Another implication of the third postulate, in particular of additivity, is that the entropy function (and the energy function) obeys the <em>Euler relation</em>, so that we can write</p>

\[\hat{S}(U, V, N_1, \ldots, N_r) = \frac{\partial{\hat{S}}}{\partial U} U + \frac{\partial{\hat{S}}}{\partial V} V + \sum_{j=1}^r \frac{\partial{\hat{S}}}{\partial N_j} N_j = \frac{1}{T} U + \frac{P}{T} V - \sum_{r=1}^n \frac{\mu_j}{T} N_j.\]

<p>There is also a fourth postulate, associated with the third law of thermodynamics, but we don’t need to concern ourselves with it in what is to follow. The important thing is that, with just these three postulates, some careful consideration of our systems of interest, and deploying the mathematical tool of <em>Legendre transformations</em>, we have a complete and well-defined theory of classical thermodynamics, as shown in Callen’s book.</p>

<h1 id="the-maximum-information-entropy-principle">The maximum information entropy principle.</h1>

<p>No, forget about classical thermodynamic entropy. Forget that I’ve ever written the first part. Pretend that this is a text on probability, and was always thus. We instead consider the following problem. Say we have a random variable $\Theta$ taking values in $\theta_1, \theta_2, \ldots$, with some probability</p>

\[\pmb{p}^{(\text{true})} = ( p_i^{(\text{true}}); \ p_i = P(\Theta = \theta_i).\]

<p>Suppose that we do not know $\pmb{p}^{(\text{true})}$, although we may know some constraints on $\pmb{p}^{(\text{true})}$. We want to then estimate some probability $\pmb{p} \{p_i\}$ such that $\pmb{p}$ is a good estimate for $\pmb{p}^{(\text{true})}$.From a Bayesian epistemological point of view, what even means for $\Theta$ to have a \pmb{p}^{(\text{true})} assigned by nobody is odd, but we will not consider this for now. We bypass this important problem for now and instead just ask</p>

<blockquote>
  <p>What is the best way to choose an probability distribution $\pmb{p}$?”, possibly under sume contraints on $\pmb{p}$, such as we will treat $\Theta$ as being distributed according to $\pmb{p}$?</p>
</blockquote>

<p>The <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">maximum entropy principle</a> gives a simple answer: for some positive constant $k_B$, choose the probability distribution $p$ such that $p$ maximizes the <em>information entropy</em></p>

\[S_I(\pmb{p}) = -k_B \sum_i p_i \log p_i\]

<p>under some possible constraints on $\pmb{p}$.</p>

<p>The idea here is that $S_I(\pmb{p})$ is a measure of the uncertainty of the probability distribution $\pmb{p}$, and you choose the distribution with as much uncertainty as possible. For finite $\theta_i, \ldots, \theta_m$, we can see that $S_I(\pmb{p})$ is a measure of uncertainty since it is minimized with value $0$ for some distribution $\pmb{p}$ that, for some $\theta_j$, assign probability one to $P(\Theta = \theta_j)$, that is, assign certainty to $\Theta = \theta_j$. Moreover, $S_I(\pmb{p})$ is maximized if $P(\theta = \theta_i) = 1/m$, that is, the distribution $\pmb{p}$ is as uniform-like, thus uncertain, as possible. Therefore, the argument goes, by maximizing $S_I(\pmb{p})$ under some constraints, we are choosing the most uninformative distribution as we possibly can.</p>

<p>A more thorough argument for $S_I(p)$ being the correct measure of uncertainty is found in E. T. Jaynes’ <a href="https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712">treatise on probability</a>, showing that the information entropy is (up to the constant $k_B$) the <em>only</em> measure of uncertainty $S_I(p)$, for an arbitrary discrete probability distribution, that satisfies:</p>

<ol>
  <li>For fixed $m$, $S_I(p_1, \ldots, p_m)$ is continuous (small changes in the distribution lead to small changes in the uncertainty), and achieves its maximum at $S_I(1/m, \ldots, 1/m)$.</li>
  <li>Letting $h(m) = S_I(1/m, \ldots, 1/m)$, $h(m)$ is monotonically increasing (so, a uniform distribution with more elements is more uncertain than one with fewer elements).</li>
  <li>$H$ is consistent in the following manner: given an ensemble of $n$ uniformly distributed elements that are divided into $k$ boxes with $b_1, …, b_k$ elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box (wording taken from <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">the Wikipedia page on information entropy</a>.</li>
</ol>

<p>Since this is true for every $\theta_1, \ldots, \theta_m$ finite, it stands to reason that it should stand for $\theta_1, \theta_2, \ldots$ countably infinite, although we do not have a uniform distribution here, and thus the distribution can only be maximized under some suitable constraint. If we take another shoddy leap, we can even assume that the possible states of $\Theta$ take place in some domain $\Omega$, and by analogy, we choose the probability density function $p(\theta)$ for $\Theta$ such that it maximizes</p>

\[S_I(p(\theta)) = -\int_\Omega p(\theta) \log p(\theta) d\theta.\]

<p>A deeper discussion on the maximum entropy principle is postponed since it is part of what I want to understand here by writing these essays.</p>

<h1 id="the-maximum-information-entropy-probability-distribution">The maximum (information) entropy probability distribution.</h1>

<p>With the maximum information entropy principle in hand, we will consider the following problem:</p>

<p>Say that we have a random variable $\Theta$ taking values in the states $\theta_1(\alpha), \theta_2(\alpha), \ldots$, possibly infinite, such that every $\theta_i$ vary continuously in some positive variable $\alpha$, whose value is given to us. We assume $\alpha$ fixed. We will later associate this variable $\alpha$ with a volume in the physical case. Suppose we do not know about the probability distribution</p>

\[\pmb{p}^{(\text{true})}(\alpha) = \{p^{(\text{true})}_1(\alpha), p^{(\text{true})}_2(\alpha), \ldots \} \\
p^{(\text{true})}_i(\alpha) := P(\Theta = \theta_i(\alpha))\]

<p>associated with the random variable $\Theta$. However, we have a set of real-value functions $\{f_j(\theta)\}_{j=1}^m$ such that we know or have measured the expected value of $f_j(\Theta)$</p>

\[\bar{f}_j = \mathbb{E}_{\Theta \sim p(\alpha)} f_j(\Theta(\alpha)) = \sum_i f_{ij}(\alpha) \\
f_{ij}(\alpha) := f_j(\theta_i(\alpha))\]

<p>for every $j=1, \ldots, m$. We want to them make a probability assignment $\pmb{p} = \{p_i\}_{i=1}$ such that $\pmb{p} \approx \pmb{p}^{(\text{true})}$. That is, we want again to answer the question</p>

<blockquote>
  <p>For some $\alpha$, what is the best way to choose an probability distribution $\pmb{p}$?”, such as we will treat some random variable $\Theta$, taking values in ${\theta_i(\alpha)}$, as being distributed according to $\pmb{p}$, such that $\pmb{p}$ should obey the constraints $\bar{f}_j = \mathbb{E}_{\pmb{p}} [f_j(\Theta)]$ associated with our measured values $\bar{f}_j$?.</p>
</blockquote>

<p>It is important to notice that, in this maximization problem, ${\bar{f}_j}$ are, together with $\alpha$, our fixed variables, since they are the results of our measurements. The principle of maximum entropy dictates that we should choose $\pmb{p}$ such that it is the solution of following the maximization problem:</p>

\[\pmb{p} = \max_{\pmb{p'}} S_I(\pmb{p'}) = -k_B \sum_{i} p'_i \log p'_i, \\
\text{s.t.} \quad \sum_{i} p'_i = 1, \\
\text{s.t.} \quad  \sum_{i}^m f_{i}(\alpha) p'_i = \bar{f}_j, \ \forall j=1, \ldots, n.\]

<p>Crucially, the probability distribution $\pmb{p}$ that maximizes this problem will be a function of ${\bar{f}_j}$ and $\alpha$. Here, $k_B$ is just a positive constant, and will not influence our maximization problem. We can use Lagrange multipliers $\lambda_0, \lambda_1, \ldots, \lambda_m$ to arrive at the equivalent system of equations</p>

\[-k_b \left(\log p_i + (1 + \lambda_0) - \sum_{j=1}^n \lambda_j f(\theta_i;\alpha)  = 0
\right) \ \forall i=1, \ldots, m\\
\sum_i p_i = 1, \\
\sum_i f_{ij}(\alpha) p_i = \bar{f}_j, \ \forall j=1, \ldots, n.\]

<p>Solving these equations, we find that</p>

\[p_i := \frac{1}{Z} e^{-\sum_{j=1}^n \lambda_j f_{ij}(\alpha)}; \\\]

<p>where $Z$ is given by</p>

\[Z = \sum_i e^{-\sum_{j=1}^n \lambda_j f_{ij}(\alpha)}.\]

<p>Finally, we find that $\lambda_1, \ldots, \lambda_n$ are found by solving the following system of equations</p>

\[\sum_i f_{ij}(\alpha) e^{-\sum_{j=1}^n \lambda_j f_{ij}(\alpha)} = \bar{f}_j \sum_i e^{-\sum_{j=1}^n \lambda_j f_{ij}(\alpha)}, \ \forall j=1, \ldots, n.\]

<p>Of course, solving this system is brutally hard. Still, if we assume the system to be uniquely solvable (we know that it is solvable at all since the Lagrange multipliers exist), we find that</p>

\[\pmb{p} = \hat{p}(Z, \pmb{\lambda}), \\
Z = \hat{Z}(\pmb{\lambda}, \alpha) = \sum_i e^{-\pmb{\lambda} \cdot \pmb{f}_i}, \\
\pmb{\lambda} = \hat{\lambda}(\pmb{\bar{f}}, \alpha),\]

<p>where we conveniently define</p>

\[\pmb{\lambda} := (\lambda_1, \ldots, \lambda_n) \\
\pmb{f}_i := (f_{i1}, \ldots, f_{in}) \\
\pmb{\bar{f}} := (\bar{f}_1, \ldots, \bar{f}_n).\]

<p>The most important of these functions will be $\hat{Z}$, which we refer to as the partition function. An important property of the partition function is that its derivatives are related to the constraints $\bar{f}_j$, and, by differentiating $\hat{Z}$ in respect to $\lambda_j$,</p>

\[\frac{\partial \hat{Z}}{\partial \lambda_j} = \sum_i -f_{ij} e^{-\pmb{\lambda} \cdot \pmb{f}_i} = \hat{Z} \bar{f}_j \implies \bar{f}_j = -\frac{\partial \log \hat{Z}}{\partial \lambda_j}.\]

<p>What about the value $S_I(p)$ found by the maximization? Defining $S_I$ as this maximum value, by plugging the value of $p$ back in the formula for $S_I(p)$, we find that,</p>

\[S = \hat{S}(\pmb{\bar{f}}, \alpha) := S(\hat{p}(\pmb{\bar{f}}, \alpha)) = k_B \left(\pmb{\lambda} \cdot \pmb{\bar{f}} + \log Z\right).\]

<p>Now, using the relation between the partial derivatives of $\log \hat{Z}$ and the expected values ${\bar{f}_j}$, we find that</p>

\[\frac{\partial \hat{S}}{\partial \bar{f}_j} = k_B \left(\lambda_j + \pmb{\bar{f}} \cdot \frac{\partial \hat{\lambda_j}}{\partial \bar{f}_j} + \frac{\partial \log \hat{Z}}{\partial \lambda_j} \frac{\partial \hat{\lambda_j}}{\partial \bar{f}_j}\right) = k_B \lambda_j \\
\frac{\partial \hat{S}}{\partial \alpha} = k_B \left(\pmb{\bar{f}} \cdot \frac{\partial \hat{\lambda_j}}{\partial \alpha} + \frac{\partial \log \hat{Z}}{\partial \lambda_j} \frac{\partial \hat{\lambda_j}}{\partial \bar{f}_j} + \frac{\partial \log \hat{Z}}{\partial \alpha} \right) = k_B \frac{\partial \log \hat{Z}}{\partial \alpha}\]

<p>Thus we find that</p>

\[S = \hat{S}(\pmb{\bar{f}}, \alpha) =  k_B \log \hat{Z} + \sum_{j=1}^m \frac{\partial \hat{S}}{\partial \bar{f}_j} \bar{f}_j, \\
d S = k_B \frac{\partial \log \hat{Z}}{\partial \alpha} dV + \sum_{j=1}^m \frac{\partial \hat{S}}{\partial \bar{f}_j} d \bar{f}_j.\]

<p>Now, this is almost Euler relation, except the term $k_B \log \hat{Z}$. If we are to make a full comparison with classical thermodynamics, we must handle this term. Before doing that, we use the theory to get a “first law of thermodynamics” for the information entropy. Assume we have only one function of the states $f(\cdot, \alpha)$, therefore only one set ${f_i}_i$ and $\bar{f}$. We find that</p>

\[\delta \bar{f} = \sum_{i} p_i \delta f_i + \sum_i f_i \delta p_i.\]

<p>If we define $\delta Q_I := \sum_i f_i \delta p_i$, we find that</p>

\[\delta \bar{f} - \bar{\delta f} = \delta Q_I.\]

<p>Moreover, since we have</p>

\[\delta S = -\sum_i \delta (p_i \log p_i) = - \sum_i \delta p_i - \sum_i \log p_i \delta p_i =
\sum_i \lambda f_i \delta p_i + \log Z \delta \sum_i p_i = \sum_i \lambda f_i \delta p_i,\]

<p>since $\delta \sum_i p_i = \delta 1 = 0$, we find that</p>

\[\frac{1}{\lambda} d S = d Q_I,\]

<p>and that</p>

<p>\(\bar{\delta f} = k_B \frac{\log \hat{Z}}{\frac \partial \alpha} dV,\)
which can also be directly derived from the properties of $\log Z$ described previously.</p>

<p>Finally, to get to Euler’s relation, we will need to <em>assume</em> that</p>

\[\log \hat{Z} \propto \alpha.\]

<p>Notice that this assumption may not hold, although the rest of the theory above is valid, we will not get Euler’s relation. Yet, assuming the above relation, we will find that,</p>

\[k \frac{\partial \log \hat{Z}}{\partial \alpha} = \frac{\partial \hat{S}}{\partial \alpha} = \text{cte},\]

<p>and arrive at Euler’s relation</p>

\[\hat{S} = \frac{\partial \hat{S}}{\partial \alpha} \alpha + \sum_{j=1}^m \frac{\partial \hat{S}}{\partial \bar{f}_j} \bar{f}_j,\]

<p>thus finding that the entropy function is extensive, so that</p>

\[\hat{S}(a \pmb{\bar{f}}, a \alpha) = a \hat{S}(\pmb{\bar{f}}, \alpha).\]

<p>That is enough for now. It should be noted that, although we assumed some enumerable set $\theta_1, \theta_2, \ldots$, the same theory also holds if we assume the continuous case of the maximum entropy principle. With that said, we go back to the physical.</p>

<h1 id="information-entropy-and-physical-entropy">Information entropy and physical entropy</h1>

<p>We go back to being a text on thermodynamics. Let us apply the maximum entropy principle to a physical situation. Suppose we are studying some macroscopic system. We know that our system has some volume $V$, and, given $V$, our macroscopic system is associated with some microscopic system $\Theta$, that at any given time takes the value of some microscopic state $\theta_1(V), \theta_2(V), \ldots$. Now, let’s assume that “being in state $\theta_i(V)$” is a random variable $\Theta$. This is reasonable to do because, from a subjective perspective, we cannot know in which state $\Theta$ for a given time. We can think of $\Theta$ as quickly jumping between the states $\theta_1(V), \theta_2(V), \ldots$ with some probability $p^{(true)}$ that we do not have access to.</p>

<p>However, suppose can measure the internal energy $U = \mathbb{E}{u(\Theta)}$, that is, thinking of $\Theta$ as jumping through states, the time-averaged measurement of the energy associated with $\Theta$. Then, we can use the maximum entropy principle. We want to make our best guess about P(\Theta = \theta_i(V)). We are under the constraint that $U = \mathbb{E} u(\Theta)$. By the maximum entropy principle, we find that</p>

\[p_i = \frac{1}{Z} e^{-\lambda U} \\
S = k_B \log Z + \lambda U
d S = k_B \frac{\partial Z}{\partial \alpha} dV + \lambda U.\]

<p>Now, renaming the variables $\lambda$ and $k_B \frac{\partial \log Z}{\partial \alpha}$ as</p>

\[\lambda = \frac{1}{T} \\
k_B \frac{\partial \log Z}{\partial \alpha} = \frac{P}{T}\]

<p>and find that</p>

\[d S = \frac{1}{T} dU + \frac{P}{T} dV,\]

<p>Finally, if we assume that $\log Z \propto V$, we find that</p>

\[S = \frac{1}{T} U + \frac{P}{T} V.\]

<p>Thus, $S$ is an extensive function of the internal energy $U$ and volume $V$, that always takes a maximum under some possible constraints (since it is the result of a maximization). If we assume $\lambda &gt; 0$ for every $U$ and $V$, it is also a monotonically increasing function of the internal energy. We arrive at all the axioms of the thermodynamic entropy, so we can equate the thermodynamic entropy $S$ with the information entropy $S$.</p>

<p>Assuming that $\lambda &gt; 0$ is the same as assuming that microstates are more unlikely the higher their energy. As for why $\log Z \propto V$, it can be proven for a variety of systems under very general assumptions, although for some systems such that this is untrue, then classical thermodynamics cannot describe this system. Those are not the main problems</p>

<h1 id="why-does-this-work-at-all">Why does this work at all?</h1>

<p>Why this works at all? We arrived at this equation by assuming the maximum information entropy principle. However, the maximum entropy principle is mostly a logical derivation or some sort of “best practice”. There is no principled reason why the true distribution $p^{(true)}$ should <em>be</em> the one we find by the maximum information entropy principle. After all, we arrive at $p^{(true)}$ through some physical process on the microstates, and this process should <em>not care at all for the maximum information entropy principle</em>. Yet, we go out there and make our measurements, and it seems that it follows exactly this principle, since we know classical thermodynamics holds.</p>

<p>So, the maximum entropy principle works, but it seems it should not. There is something to be investigated here. This will be the focus of the second part.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Updates]]></summary></entry><entry><title type="html">Gravitational Golf - A game with GPT-4</title><link href="http://localhost:4000/2023/03/15/gpt4-game.html" rel="alternate" type="text/html" title="Gravitational Golf - A game with GPT-4" /><published>2023-03-15T20:00:00-03:00</published><updated>2023-03-15T20:00:00-03:00</updated><id>http://localhost:4000/2023/03/15/gpt4-game</id><content type="html" xml:base="http://localhost:4000/2023/03/15/gpt4-game.html"><![CDATA[<p><a href="https://openai.com/research/gpt-4">GPT-4 just launched yesterday</a>, if you didn’t already knew of it. I used it to
create a minigame and I’m truly amazed. I acted mostly as a supervisor than anything,
because the code almost in its entirety is due to GPT-4.</p>

<p>Sometimes GPT-4 commited errors, but usually he himself corrected it. Of course,
here and there I corrected some errors myself, but they where minor. It was honestly
95% GPT and 5% me.</p>

<p>I’m kind of speechless, if I am to be honest. Just leaving the game here. It is on the “Golf” tab.</p>

<h2 id="update-1">UPDATE 1</h2>
<p>Although the title is Gravitational Golf, 
it is working more as a Gravitational Soccer. But it is still great.</p>

<h2 id="update-2">UPDATE 2</h2>
<p>I found an small error, and surprisingly, it was the very small part that I’ve edited. This is humbling.</p>

<h2 id="update-3">UPDATE 3</h2>
<p>Screw that, I made another web application again using GPT-4. This time I had even less working setting it up, I think I got better at how to ask GPT-4 for things. All of those can be found on the Apps tab.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[GPT-4 just launched yesterday, if you didn’t already knew of it. I used it to create a minigame and I’m truly amazed. I acted mostly as a supervisor than anything, because the code almost in its entirety is due to GPT-4.]]></summary></entry><entry><title type="html">A technical tutorial on Large Language Models - Interlude on Reinforcement Learning</title><link href="http://localhost:4000/2023/01/26/llm-tutorial-2.html" rel="alternate" type="text/html" title="A technical tutorial on Large Language Models - Interlude on Reinforcement Learning" /><published>2023-01-26T12:00:00-03:00</published><updated>2023-01-26T12:00:00-03:00</updated><id>http://localhost:4000/2023/01/26/llm-tutorial-2</id><content type="html" xml:base="http://localhost:4000/2023/01/26/llm-tutorial-2.html"><![CDATA[<p><a href="/2023/01/14/llm-tutorial-1.html">Part 1</a></p>

<h1 id="update-2306">Update (23/06)</h1>

<p>I turned this into a general interlude for RL and RLHF, and I decided to write the next part of actually applying this to language models.</p>

<h1 id="update-2405">Update (24/05)</h1>

<p>This text has been revised and edited by ChatGPT 3.5 to improve grammar and overall structure.</p>

<h1 id="preamble">Preamble</h1>

<p>In Part 1, I mentioned that we would delve into ChatGPT. However, I must confess that I slightly misled you. The exact training details of ChatGPT have not been published yet. OpenAI has provided only a <a href="https://openai.com/blog/chatgpt/">general overview</a>. Nonetheless, OpenAI claims that the setup of ChatGPT is highly similar to the one used in the <a href="https://openai.com/blog/instruction-following/">InstructGPT</a> series. Therefore, we will rely on the information presented in the <a href="https://arxiv.org/pdf/2203.02155.pdf">original InstructGPT paper</a>. However, to comprehend InstructGPT, we first need to understand both supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Yet, to do that, we need to understand reinforcement learning, which is the point of this interlude.</p>

<h1 id="reinforcement-learning---a-primer">Reinforcement learning - A primer.</h1>

<p>To comprehend reinforcement learning from human feedback (RLHF), it is essential to grasp the fundamentals of reinforcement learning. For a more detailed introduction, refer to the <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">OpenAI tutorial</a>. Here, I provide a concise overview.</p>

<h2 id="the-building-blocks">The building blocks.</h2>

<p>Consider an agent that interacts with an environment through a sequence of steps. The environment begins in a state \(s_0 \in \mathcal{S}\) with probability \(p(s_0)\). At each step \(t\), the agent receives an observation \(o_t \in \mathcal{O}\) and a reward \(r_t\) based on a reward function of the form \(r_t = r(a_t, o_t)\). The agent takes an action \(a_t\), causing the environment to transition from state \(s_t\) to another state \(s_{t+1}\) with a probability \(p(s_{t+1} \mid s_t, a_t)\) (not necessarily known to the agent). The observation \(o_t\) is also dependent on the environment’s state, given by \(s_t \sim p(o_t \mid s_t)\). If \(o_t = s_t\), the environment is <em>fully observable</em>; otherwise, it is <em>partially observable</em>. Notably, if we have a prior \(p(s)\) on the state, we can expand \(p(o_{t+1} \mid o_t,a_t)\) as follows:</p>

\[p(o_{t+1} \mid o_t,a_t) = \int p(o_{t+1} \mid s_{t+1})p(s_{t+1} \mid a_t,o_t) ds_{t+1} \\
p(s_{t+1} \mid a_t,o_t) = \int p(s_{t+1} \mid a_t,s_t)p(s_t \mid o_t) ds_t \\
p(s_t \mid o_t) = \frac{p(o_t \mid s_t) p(s_t)}{\int p(o_t \mid s_t) p(s_t) ds_t}.\]

<p>The agent’s actions are guided by a (possibly probabilistic) <em>policy</em> \(\pi(a \mid o_t)\), where \(a_t \sim \pi(a_t \mid o_t)\). In the context of neural networks, policies are often denoted as \(\pi_\theta\), incorporating parameters. Let the agent begin at \(t=0\) and interact with the environment for \(T\) time steps until termination, constituting a single <em>episode</em>. The probability of the agent following a <em>trajectory</em> \(\tau = \{o_0, a_0, o_1, a_1, \ldots, o_T\}\) is given by</p>

\[p_{\pi}(\tau) = p(o_0) \prod_{i=1}^T p(o_{t+1} \mid o_t,a_t)\pi(a_t \mid o_t).\]

<p>A trajectory is associated with a cumulative reward \(R(\tau)\), which can be the cumulative sum for a <em>finite horizon</em> of \(T &lt; \infty\) time steps:</p>

\[R(\tau) = \sum_{t=0}^{T-1} \gamma^t r_t,\]

<p>where \(\gamma \in (0, 1]\) is the discount rate. Alternatively, for \(T=\infty\) and \(\gamma \leq 1\), the <em>infinite horizon</em> cumulative reward is given by</p>

\[R(\tau) = \sum_{t=0}^\infty \gamma^t r_t.\]

<p>It is important to note that an episode does not necessarily run indefinitely, and if it does, we must ensure \(\gamma &lt; 1\). We also define future cumulative rewards starting at time step \(t\) as follows:</p>

\[R_t(\tau) = \sum_{s=t}^{T-1} r_t, \quad R_t(\tau) = \sum_{s=t}^{\infty} \gamma^s r_t.\]

<p>Thus, for a policy \(\pi\), the expected cumulative reward \(J(\pi)\) is given by</p>

\[J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)}[R(\tau)].\]

<p>The <em>reinforcement learning objective</em> is to find a policy \(\pi(a \mid o)\) that maximizes \(J(\pi)\). In practice, we employ some form of gradient descent to minimize \(-J(\theta) = -J(\pi_\theta)\) for a parameterized policy \(\pi_\theta\), using an estimate of \(\nabla J(\theta)\). The next section, which delves into optimization algorithms, is optional as RLHF can be understood without exploring specific optimization algorithms. These algorithms typically employ minibatch gradient descent over trajectories.</p>

<h2 id="value-functions">Value functions.</h2>

<p>Before delving into optimizing \(J(\pi)\), let’s define some key components associated with the policy \(\pi(a \mid o)\). First, we have the <em>value function</em> \(V_\pi(o)\), which represents the expected cumulative reward of following policy \(\pi\) starting from observation \(o\). It is given by</p>

\[V_{\pi}(o) = \mathbb{E}_{\tau \sim p_\pi(\tau)}[R(\tau) \mid o_0=o],\]

<p>where \(R(\tau)\) is the cumulative reward obtained from trajectory \(\tau\). Similarly, the <em>action-value function</em> \(Q_{\pi}(o, a)\) represents the expected cumulative reward of following policy \(\pi\) after starting from observation \(o\) and taking action \(a\):</p>

\[Q_{\pi}(o,a) = \mathbb{E}_{\tau \sim p_\pi(\tau)}[R(\tau)  \mid o_0=o, a_0=a].\]

<p>We also have the relationship \(V_\pi(o) = \mathbb{E}_{a \sim p(a \mid o)}[Q_\pi(o,a)]\). For infinite time horizons, both \(V_\pi(o)\) and \(Q_\pi(o, a)\) satisfy the <em>Bellman equations</em>:</p>

\[V_\pi(o) = \mathbb{E}_{o', a \sim p(o' \mid o,a)\pi(a \mid o)}[r(a, o) + \gamma V_\pi(o')], \\
Q_\pi(o, a) = \mathbb{E}_{o' \sim p(o' \mid o,a)}[r(a, o) + \gamma \mathbb{E}_{a' \sim p(a' \mid o')}[Q_\pi(o',a')]],\]

<p>where \(\gamma\) is the discount factor. These equations capture the notion that the value or action/value from a state or action is the immediate expected reward plus the discounted future value of the expected future state. In finite time horizons, the situation becomes more complex as we need to consider the timing within the episode. However, by incorporating this information in our observations \(o_t\), the Bellman equations remain valid for finite horizons. Another important concept is the advantage function:</p>

\[A_\pi(o, a) = Q_\pi(o,a) - V_\pi(o),\]

<p>which quantifies the advantage of taking action \(a\) in state \(o\) followed by policy \(\pi\) compared to simply following policy \(\pi\) from state \(o\).</p>

<h2 id="solving-reinforcement-learning---offline">Solving reinforcement learning - offline.</h2>

<p>There are two main approaches to reinforcement learning: <em>online</em> reinforcement learning and <em>offline</em> reinforcement learning. In online RL, the agent updates its policy during the episode, similar to how humans and animals learn. However, in machine learning settings like ChatGPT, we typically employ <em>offline</em> RL. In offline RL, the agent plays multiple episodes \(\tau_i \sim p_{\pi_\theta}(\tau)\) using the current policy \(\pi_\theta\), observes the rewards \(\left(r^{(i)}_1, \ldots, r^{(i)}_{T} \right)\), and then updates \(J(\theta)\) using the stochastic approximation:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)}[R(\tau)] \approx \frac{1}{N} \sum_{i=1}^N R(\tau_i).\]

<p>To compute the gradient \(\nabla J(\theta)\), we encounter a challenge because the trajectories \(\tau_i\) are not directly differentiable with respect to the policy parameters \(\theta\). However, a clever trick allows us to overcome this issue. The derivation and more details can be found <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html">here</a>, but the key result is:</p>

\[\nabla J(\theta) = \mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)}\left[ \sum_{t=1}^{T} R(\tau) \nabla_\theta \log \pi_\theta(a_t \mid o_t)\right],\]

<p>which leads to the following approximation:</p>

\[\nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left[ \sum_{t=1}^{T} R(\tau_i) \nabla_\theta \log \pi_\theta(a^{(i)}_t \mid o^{(i)}_t)\right].\]

<p>In practice, we can substitute \(R(\tau_i)\) with \(R_t(\tau_i)\) and further replace \(R_t(\tau_i)\) with \(R_t(\tau_i) - b(s_t)\), where \(b(s_t)\) is a function that only depends on the state. We can approximate \(b(s_t)\) using a neural network \(V_\phi(s_t)\), which estimates the true value function \(V_{\pi_\theta}(s_t)\). The estimation is done by minimizing the following residual:</p>

\[V_\phi(s_t) = \argmin_\phi \sum_{i=1}^N \sum_{t=1}^T (V_\phi(s^{(i)}_t) - R_t(\tau_i))^2\]

<p>using stochastic gradient descent. These substitutions improve the stability of RL optimization. An annotated example of this algorithm applied to the CartPole environment can be found in <a href="https://colab.research.google.com/drive/1lz_FFlWFOexvU_LY80O5qqJGllPE_Ryo?usp=sharing">this notebook</a> I created earlier for other purposes.</p>

<p>The field of offline RL encompasses various algorithms, and in the case of InstructGPT, an algorithm called <em>proximal policy optimization</em> (PPO) is used. More details about PPO can be found <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">here</a>. However, the important point is not the specific RL algorithm being used, but rather understanding the underlying problem being addressed. This understanding will serve as the foundation for the next step: <em>reinforcement learning from human feedback</em>.</p>

<h1 id="from-rl-to-rlhf">From RL to RLHF.</h1>

<p>Having gained an understanding of reinforcement learning, we can now explore reinforcement learning from human feedback (RLHF). The key insight of RLHF is that the agent no longer has access to the true reward signal \(r_t\); instead, it must rely on a <em>model</em> of the reward function, which is obtained through feedback from humans. In this section, we will follow the explanation of RLHF provided by <a href="https://arxiv.org/pdf/1706.03741.pdf">Christiano et al.</a>.</p>

<p>Let’s consider that the agent has a model of the reward function \(\hat{r}(a, o)\), which is created based on human feedback. Sometimes, we use the subscript \(\phi\) (e.g., \(\hat{r}_\phi\)) to indicate that a neural network models the reward function. It’s important to note that \(\hat{r}\) represents a reward function that fits human preferences, rather than the true underlying human reward function, which can be more subjective.</p>

<p>We define the probability of a human preferring trajectory \(\tau\) over trajectory \(\tau'\) as:</p>

\[p(\tau \succ \tau' \mid \hat{r}) = \sigma(\hat{R}(\tau) - \hat{R}(\tau')),\]

<p>where \(\sigma\) is the sigmoid function. Similarly, we assume that \(p(\tau \prec \tau' \mid \hat{r}) = 1 - p(\tau \succ \tau' \mid \hat{r})\), meaning that we do not model indifference or non-comparability between trajectories.</p>

<p>Suppose we have a dataset of ordered pairs of trajectories \(\mathcal{D} = \{(\tau_i, \tau_i^\prime)\}_i\), where each pair \((\tau_i, \tau_i^\prime)\) represents a preference judgment made by a human (e.g., \(\tau_i \succ \tau_i^\prime\)). We construct our loss function for \(\hat{r} = \hat{r}_\phi\) as:</p>

\[l(\phi;\mathcal{D}) = \mathbb{E}_{(\tau, \tau^\prime) \sim \mathcal{D}}[-\log p(\tau \succ \tau' \mid \phi)].\]

<p>This loss is minimized using minibatch gradient descent, and the resulting learned reward function \(\hat{r}_\phi\) is used to update the agent’s policy \(\pi_\theta\) by maximizing \(J(\theta;\phi)\). This creates a cyclic process involving multiple steps:</p>

<ol>
  <li>Sample trajectories \(\mathcal{D}^u = \{\tau^u_k\}_k\) from the current policy \(\pi_\theta\).</li>
  <li>Collect human preferences based on \(\mathcal{D}^u\) to obtain trajectory preference pairs \(\mathcal{D} = \{(\tau_i, \tau_i’)\}_i\).</li>
  <li>Learn the reward function \(\hat{r}_\phi\) by minimizing the loss \(l(\phi;\mathcal{D})\) using minibatch gradient descent.</li>
  <li>Optimize the policy \(\pi_\theta\) using the learned reward function \(\hat{r}_\phi\) through a reinforcement learning algorithm that maximizes \(J(\pi;\hat{r}_\phi)\).</li>
  <li>Repeat from step 1.</li>
</ol>

<p>Next, we will use that general framework to explain instruct-based language models, and how they are crafted from base models.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 1]]></summary></entry><entry><title type="html">Visualizing Layer Normalization</title><link href="http://localhost:4000/2023/01/15/visualizing-layer-normalization.html" rel="alternate" type="text/html" title="Visualizing Layer Normalization" /><published>2023-01-15T17:49:16-03:00</published><updated>2023-01-15T17:49:16-03:00</updated><id>http://localhost:4000/2023/01/15/visualizing-layer-normalization</id><content type="html" xml:base="http://localhost:4000/2023/01/15/visualizing-layer-normalization.html"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>Here, I explore some geometric intuition of layer normalization, that are found in transformers. I’ve been in general trying to “imagine” operations on transformers. My “visualization” of multi-head attention largely follows the insights from <a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</a>, while I imagine the feedforward neural network part as “warping the space” of embedding vectors. I struggled a bit with layer normalization until I realized that it had a quite nice interpretation. I’m sharing this here in case anyone is interested.</p>

<h1 id="visualizing-layer-normalization">Visualizing layer normalization</h1>

<p>In a neural network, the layer norm operation on a vector \(x \in \mathbb{R}^n\) is the operation defined by</p>

\[\operatorname{lnorm}(x;a,b) = a \odot \frac{x - \operatorname{mean}(x)}{\operatorname{std}(x) + \epsilon} + b \\ \quad \\
\operatorname{mean}(x) = \frac{1}{n} \sum_{i=1}^n x_i \\ \quad \\
\operatorname{std}(x) = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \operatorname{mean}(x))^2}\]

<p>where \(a, b \in \mathbb{R}^n\) are parameters of the layer norm, and \(\epsilon\) is an small stability factor (for instance \(\epsilon = 10^{-12}\) is valid). Here, \(\odot\) denotes the element-wise product. Layer normalization is more interpretable if it can be seen as the composition of three functions</p>

\[\operatorname{lnorm}(x;a,b) = h(g(f(x));a, b) \\
\quad\\
f(x) = \left(I_{n} - \frac{1}{n} \mathbf{1}_{n \times n} \right) x \\
\quad \\
g(x) = \sqrt{n} \frac{x}{\|x\|_2 + \epsilon \sqrt{n}} \\
\quad \\
h(x;a,b) = \operatorname{diag}(a)x+b ,\]

<p>where \(I_{n}\) is the \(n\)-dimensional identity matrix, and \(\mathbf{1}_{n \times n}\) is the \(n\)-dimensional matrix of ones. We can interpret each of these in turn.</p>

<p>We begin with \(f\). First, consider the matrix \(P = \frac{1}{n} \mathbf{1}_{n \times n}\). It is not hard to see both the vector of ones \(\mathbf{1}_n\) is an eigenvector of \(P\) with eigenvalue \(\lambda_1 = 1\). Moreover, the null space of \(P\) must be of dimension \(n-1\), so it forms an eigenspace with eigenvalue being 0, that is orthogonal to \(\mathbf{1}_n\) (since \(P\) is symmetric). Therefore, \(P\) is an orthogonal on the 1-dimensional subspace \(\{x; x_1=x_2=\ldots=x_n\}\), generated by \(\mathbf{1}_n\). It follows that \(I - P\) is an orthogonal projection on the \((n-1)\)-dimensional subspace orthogonal to \(\mathbf{1}_n\), which we denote by \(U\). Therefore, \(f\) is an orthogonal projection into \(U\).</p>

<p>Going on with \(g\), we have that, if \(\|x\|_2 \gg \epsilon\), which will be true except for vectors very close to the origin, \(g = \sqrt{n} x/\|x\|_2\) is just a projection into a \((n-1)\)-dimensional sphere of radius \(\sqrt{n}\), which we denote \(\sqrt{n}S_{n-1} \in \mathbb{R}^n\). Since \(f(x)\) already projects \(x\) to \(U\), it means that \(g(f(x))\) actually lives on an \((n-2)\)-dimensional sphere contained in \(U\). We denote this sphere by \(\sqrt{n} S_{n-2, U} = S_{n-1} \cap U\).</p>

<p>Finally, \(h(x; a, b)\) is an scaling by \(A = \operatorname{diag}{a}\) and change of location by \(b\). Notice that, since our vectors \(x\) are living in \(\sqrt{n} S_{n-2, U}\) after the composition \(g \circ f\), \(A\) not only distorts \(\sqrt{n} S_{n-2, U}\) into an elipsis, but actually takes \(U\) into a different subspace generated by applying \(A\) on \(U\), and then leads it to an <strong>**</strong>affine<strong>**</strong> space, in an ellipsis centered in \(b\).</p>

<p>In summary, \(\operatorname{lnorm}\) project an embedding vectors \(x \in \mathbb{R}^n\) into a \((n-2)\) dimensional ellipsis, whose shape and location is determined by \(a\) and \(b\).</p>

<h1 id="what-about-rms-layer-normalization">What about RMS Layer Normalization</h1>

<p>This transformation in an ellipsis seems needlessly complicated. What if we instead projected the embedding vectors directly in a sphere, and manipulated that sphere if necessary? This is the intuition of <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>, which sheds the operation \(f\) above, and lets \(b=0\). This way, \(g\) projects the embedding vectors \(x\) into the sphere \(\sqrt{n} S_{n-1}\), and \(a\) just warps the principal axes of this sphere (on the standard basis) to the size \(\sqrt{n} a_i\). That makes interpretation at least way easier. Unfortunately, in the wild, we have mostly layer normalization, so the above interpretation is still the most important one.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">A technical tutorial on Large Language Models - Part 1</title><link href="http://localhost:4000/2023/01/14/llm-tutorial-1.html" rel="alternate" type="text/html" title="A technical tutorial on Large Language Models - Part 1" /><published>2023-01-14T16:11:16-03:00</published><updated>2023-01-14T16:11:16-03:00</updated><id>http://localhost:4000/2023/01/14/llm-tutorial-1</id><content type="html" xml:base="http://localhost:4000/2023/01/14/llm-tutorial-1.html"><![CDATA[<p><a href="/2023/01/26/llm-tutorial-2.html">Part 2</a></p>

<h1 id="update-2405">Update (24/05)</h1>

<p>This text has been revised and edited by ChatGPT 3.5 to improve grammar and overall structure.</p>

<h1 id="update-2505">Update (25/05)</h1>

<p>Some links that I recommend, that are likely better than what I’ve written here:</p>

<ul>
  <li><a href="https://github.com/jacobhilton/deep_learning_curriculum/blob/master/1-Transformers.md">Chapter 1: Transformers</a> of Jacob Hinton’s deep learning curriculum. Also, all the links inside it. I <em>strongly</em> recommend doing the exercise, which is exactly what I’ve done below.</li>
  <li><a href="https://transformer-circuits.pub/2021/framework/index.html">Transformer Circuits</a> by Anthropic, in particular the “Transformer Overview” section.</li>
  <li><a href="https://arxiv.org/abs/2005.14165">GPT-3 Paper</a> and <a href="https://arxiv.org/abs/2203.02155">InstructGPT Paper</a>.</li>
</ul>

<h1 id="preamble">Preamble</h1>

<p>While delving deeper into the study of language models, I am currently following the curriculum developed by Jacob Hilton, which can be found at <a href="https://github.com/jacobhilton/deep_learning_curriculum">this GitHub repository</a>. The initial exercise within this curriculum involves building a language model from scratch and training it on the works of Shakespeare.</p>

<h1 id="what-is-this-tutorial">What is this tutorial?</h1>

<p>This is the first part of a tutorial on language models, specifically focusing on GPT models. It is a relatively concise tutorial that assumes the reader has <em>some</em> background knowledge in deep learning, and I make use of mathematical notation throughout.</p>

<p>This part guides us on the journey from nothing to “pure” GPT models, which serve as the foundation for language models like ChatGPT (those are also called base models or foundation models). In the next part, we will delve into the transition from “pure” models to fine-tuned models such as ChatGPT. This tutorial provides operational instructions, explaining <em>how</em> we arrive at specific language models. The reasons behind their functioning will be explored in a future post.</p>

<p><img src="/assets/figs/lm1/postdiagram.png" alt="postdiagram.png" /></p>

<p>My primary motivation here is to learn through writing, while also addressing the lack of comprehensive transformer tutorials. It is quite astonishing that there are so few well-crafted tutorials or explanations for what is arguably the most significant revolution in deep learning in the past decade. This tutorial is another, albeit opinionated, attempt to fill that gap and may prove useful to some.</p>

<p>Now, you might wonder why you should care. I might provide an introduction later, but to be honest, simply experimenting with <a href="https://chat.openai.com/chat">ChatGPT</a> for a while should be enough to motivate your interest in language models.</p>

<h1 id="language-models---evaluation">Language models - evaluation.</h1>

<p>Operationally, a language model can be thought as following the rule “given string \(s\) of natural language, output string \(w\) with probability \(p(w\mid s)\). This is what GPT-like models do at the end of the day. So the question is how to do that? For instance, when writing \(s = \text{``I ate a ''}\), the model outputs \(w = \text{``banana at the market''}\) with probability \(p(w\mid s) = 0.8\), as an example. The point is how the model does that?</p>

<p>First, we need to map \(s\) and \(w\) into some suitable structure for our model. We do this by making use of a “sequencing function” \(T\) that will map strings \(s\) to finite sequences of integers \(\mathbf{x} = (x_1, \ldots, x_n)\), with elements belonging to a finite set \([n_{vocab}] = \{1, \ldots, n_{vocab}\}\) which we call the <em>vocabulary</em>, \(n_{vocab}\) being the <em>vocabulary size,</em> that is, the “words” that the sequence model knows. We reserve \(1\) for the “end-of-sentence” token, that will be explained below. As an example, we transform the sequence \(s = \text{``I ate a''}\) into a sequence \(\mathbf{x} = (10, 123, 2)\).</p>

<p>Now, we transformed our language evaluation problem into an equivalent <em>sequence</em> evaluation problem, as follows: let \(\mathbf{x} = (x_1, \ldots, x_n)\) be a sequence in our vocabulary \([n_{vocab}]\). Our sequence model will output another finite sequence \(\mathbf{y}=(y_1, \ldots, y_m)\) with probability \(p(\mathbf{y}\mid \mathbf{x})\). Now, we can turn this sampling problem into a <em>next-item</em> prediction as follows: consider \(\mathbf{y}\) be a \(m\)-sized sequence as above, and consider \(1\) to signal the end of a sequence. Then, starting with \(\mathbf{y} = ()\), we will, repeatedly,</p>

<ul>
  <li>Sample \(y_{i+1}\) from \(p_M(y\mid x_{1},\ldots, x_n, y_1, \ldots, y_i)\). If \(y_{i+1} = 1\) stop, and return \(\mathbf{y} = (y_1, \ldots, y_i)\).</li>
  <li>Update \(\mathbf{y}\) to \(\mathbf{y} = (y_1, \ldots, y_i, y_{i+1})\).</li>
</ul>

<p>That is, we use the chain rule of probability to sample \(\mathbf{y}\) with probability letting \(\mathbf{y}^i = (y_1, \ldots, y_i)\), and letting \(\mathbf{x}\mathbf{y}\) be the concatenation of two sequences, we find that</p>

\[p(\mathbf{y}\mid \mathbf{x}) = p_M(1\mid \mathbf{x}\mathbf{y})\prod_{i=1}^{n-1} p_M(y_{i+1}\mid \
\mathbf{x}\mathbf{y}^i),\]

<p>with \(\mathbf{y}^i := (y_1, \ldots, y_i)\), and \(\mathbf{x}\mathbf{y}\) denoting the concatenation of two sequences \(\mathbf{x}\) and \(\mathbf{y}\). Therefore, our model \(M\) will only output a <em>single</em> number \(y \in [n_{vocab}]\), given a sequence \(\mathbf{x} = (x_1, \ldots, x_n)\) also in \([n_{vocab}]\), with probability \(p_M(y\mid \mathbf{x})\). The chain rule will take care of the rest, and such a model will be called an <em>autoregressive</em> model.</p>

<p>Finally, given the sequence \(\mathbf{y}\), we can unsequence it to get string \(w\). We do that by applying an inverse sequencing function \(T^{-1}\) to each element of \(\mathbf{y}\), getting a string \(w = T^{-1}(\mathbf{y})\). Thus, our language model consists of a sequence model \(M\) predicting the next token, and a sequencing/unsequencing pair \((T, T^{-1})\). We have \(p_{M, T}(w\mid s) = p_M(T(w)\mid T(s))\), and we sample \(w\) by sequencing \(s\) into \(\mathbf{x}\), sampling \(\mathbf{y}\) from \(M\) given \(\mathbf{x}\), and unsequencing \(\mathbf{x}\) into \(w\).</p>

<p>An example of how this works is transforming \(s = \text{"I ate a"}\) into \(\mathbf{x} = (10, 123, 2)\), sampling \(12 \sim p_M(y\mid 10, 123, 2)\), \(4 \sim p_M(y\mid 10, 123, 3, 12)\), \(7 \sim p_M(y\mid 10, 123, 3, 2, 4)\), \(71 \sim p_M(10, 123, 3, 2, 4, 7)\), \(1 \sim p_M(10, 123, 3, 2, 4, 7, 71)\), and unsequencing \(\mathbf{y} = (12, 4, 7, 71)\) into \(w = \text{"banana at the market"}\). Next, we show how this works in a code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Write an autocomplete function and evaluate on a model
#trained on the complete works of Shakespeare.
</span><span class="k">def</span> <span class="nf">autocomplete</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">string</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
		<span class="c1">#The next line is responsible for the sequencing
</span>    <span class="n">base</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">vocab</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">string</span><span class="p">))).</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
    <span class="n">nmax</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">nctx</span> <span class="c1">#We guarantee our model not surpasses context window
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="o">-</span><span class="n">nmax</span><span class="p">:])[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1">#Output the log-probabilities
</span>        <span class="n">ind</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#Sample from output
</span>        <span class="n">base</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">base</span><span class="p">,</span> <span class="n">ind</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#Append to our sequence
</span>        <span class="k">if</span> <span class="n">ind</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#Here we are using 0 instead of 1 for end-of-senence
</span>            <span class="k">break</span>
		<span class="c1">#The next line is responsible for the desequencing
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">detokenizing</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">lookup_tokens</span><span class="p">(</span><span class="n">base</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">tolist</span><span class="p">()))</span>
		<span class="k">return</span> <span class="n">output</span>
<span class="k">print</span><span class="p">(</span><span class="n">autocomplete</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">'QUEEN OF WALES'</span><span class="p">))</span>
<span class="s">"""
 QUEEN OF WALES
 RICHARD, and KEEPER at Bordeaux.
 I have leisure kept together,
 Which of twelve thousand looks in France,
 Against the French Duke of Norfolk,
 Whose power of Hereford, Donalbain, speed,
 Under your progenitors, charming arms, Edward’s blood,
 And care not his father lives at MALCOLM.
 It may not be discharged truth,
 Where let, Harry to prevent enacted ships
 To pay, and princes if can be bold
 But must suspect ere we bid be alive
 To follow them to the King with passage.
 My noble lord, prince. I have sounded London,
 So many the living time
"""</span>
</code></pre></div></div>

<h1 id="sequencing-tokenizing-and-indexing">Sequencing, tokenizing and indexing.</h1>

<p>Now, the task of transforming a string into a sequence of integers, which we will call <em>identifiers</em> or <em>ids</em>, consists of two parts:</p>

<ul>
  <li>Splitting the string into words and subwords, which we refer to as <em>tokens</em>, using a <em>tokenization</em> rule. For example, transforming “I love you” into (“I”, “love”, “you”).</li>
  <li>Using a lookup table to transform a sequence of <em>tokens</em> into a sequence of identifiers by assigning a unique <em>identifier</em> to each token in our <em>vocabulary</em>, which is our list of known tokens.</li>
</ul>

<p>The second part is relatively straightforward. We associate each token in our <em>vocabulary</em> with a unique <em>identifier</em> and use this lookup table to convert our sequence of tokens into a sequence of identifiers. In cases where a token is not found in our vocabulary, we can assign it a special unknown identifier.</p>

<p>The first part, tokenization, is more complex, and various tokenization algorithms exist. A comprehensive tutorial can be found <a href="https://huggingface.co/docs/transformers/tokenizer_summary">here</a>. However, for now, let’s assume that each word and each sentence is treated as a token (which is a valid tokenization approach). When mapping a sequence back, we can simply invert our lookup table, associating the “unknown” identifier with a blank sentence or a <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> symbol, and concatenate our tokens according to a specific rule (such as the regular grammar rule for joining words and punctuation). Below, we provide an example of tokenization using <a href="https://spacy.io">spaCy</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Here, we create a vocabulary from the works of Shakespeare,
#and use the Spacy tokenizer.
</span><span class="n">site</span> <span class="o">=</span> <span class="s">"https://www.gutenberg.org/files/100/100-0.txt"</span>
<span class="n">data_string</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">site</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
<span class="c1">#Tokenizer our data using Spacy tokenizer, to create a vocabulary
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">torchtext</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">get_tokenizer</span><span class="p">(</span><span class="s">'spacy'</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data_string</span><span class="p">)</span>
<span class="c1">#Create a vocabulary from tokens
</span><span class="n">sorted_tokens_freq</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">OrderedDict</span><span class="p">(</span>
                        <span class="nb">sorted</span><span class="p">(</span><span class="n">collections</span><span class="p">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">).</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">torchtext</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">vocab</span><span class="p">(</span><span class="n">sorted_tokens_freq</span><span class="p">,</span> <span class="n">specials</span><span class="o">=</span><span class="p">[</span><span class="s">'&lt;unk&gt;'</span><span class="p">])</span>
<span class="n">vocab</span><span class="p">.</span><span class="n">set_default_index</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="s">'&lt;unk&gt;'</span><span class="p">])</span>
<span class="c1">#Create a detokenizing function
</span><span class="k">def</span> <span class="nf">detokenizing</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[(</span><span class="s">' '</span> <span class="o">+</span> <span class="n">s</span> <span class="k">if</span> <span class="n">s</span><span class="p">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="k">else</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="s">"hello my friend"</span><span class="p">))</span>
<span class="c1">#--&gt; ['hello', 'my', 'friend']
</span><span class="k">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="s">"hello my friend"</span><span class="p">)))</span>
<span class="c1">#--&gt; [0, 15, 292]
</span><span class="k">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">lookup_tokens</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="c1">#--&gt; ['the', 'is', 'of']
</span><span class="k">print</span><span class="p">(</span><span class="n">detokenizing</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">lookup_tokens</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])))</span>
<span class="c1">#--&gt; the is of
</span></code></pre></div></div>

<h2 id="training-the-sequence-model">Training the sequence model.</h2>

<p>Now, our goal is to have our model \(M\) as a function that, for a given evaluation sequence \(\mathbf{x}^{eval} = (x_1^{eval}, \ldots, x_{n_{eval}}^{eval})\), outputs \(p(y\mid \mathbf{x}^{eval})\) for each \(y = 1, 2, \ldots, n_{vocab}\). This allows us to sample \(y_{n_{eval}}^{eval} \sim p(y\mid \mathbf{x}^{eval})\). To achieve this through supervised learning, we need training pairs \((\mathbf{x}^{train}, y^{train})\) to train our model. Let’s consider a single sentence \(\mathbf{x} = (x_1, \ldots, x_n)\) as our training data. From this, we can infer that \((x_1)\) should output \((x_2)\), \((x_1, x_2)\) should output \(x_3\), and so on. Therefore, we end up with \(n-1\) training pairs of the form \((\mathbf{x}^i, x_{i+1})\) for \(i = 1, \ldots, n-1\). We will use this fact to design a neural network \(f_\theta\) that takes finite sequences \(\mathbf{x} = (x_1, \ldots, x_n)\) as input and outputs an \(n \times n_{vocab}\)-sized matrix \(f_\theta(\mathbf{x})\), where the elements are given by \(f_\theta(\mathbf{x})_{i, j} = p(j\mid \mathbf{x}^i)\).</p>

<p>Next, we need to define a loss function. For a given sequence \(\mathbf{x}\), we can use the negative log-likelihood to define our loss function \(l\) for parameters \(\theta\) as</p>

\[l(\theta;\mathbf{x}) = -\sum_{i=1}^{n-1} \log p(x_{i}\mid \mathbf{x}^{i}) = -\sum_{i=1}^{n-1} \log f_\theta(\mathbf{x})_{i, x_{i}+1}\]

<p>To ensure that this approach works, we need to make sure that when passing \(\mathbf{x}\) to \(f_\theta\), the output \(f_\theta(\mathbf{x})_{i}\) <em>only depends on</em> \(\mathbf{x}^i\) and does not incorporate any information from subsequent tokens. In other words, it should not use any information from \(x_{i+1}\) itself or from “future” positions, as that would be considered “cheating.”</p>

<p>For now, let’s imagine \(f_\theta(\mathbf{x})\) as a “magic transformer box.” The schematics of our language model work as shown in the following figure, which predicts with 70% chance that the sentence ends after the phrase “apple banana banana”.</p>

<p><img src="/assets/figs/lm1/diagram4.png" alt="diagram4.png" /></p>

<p>In the next steps, we will apply this “magic transformer box” philosophy to train the GPT model using the complete works of Shakespeare (actually, just 80% of them).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Create our train and text dataloader.
</span><span class="k">def</span> <span class="nf">batchify</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">line_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">nbatches</span> <span class="o">=</span> <span class="n">tensor_size</span><span class="o">//</span><span class="n">line_size</span>
    <span class="n">overhead</span> <span class="o">=</span> <span class="n">line_size</span> <span class="o">-</span> <span class="n">tensor_size</span><span class="o">%</span><span class="n">line_size</span>
    <span class="k">if</span> <span class="n">overhead</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">overhead</span><span class="p">))</span>
        <span class="n">nbatches</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="n">line_size</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">new_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
<span class="n">line_size</span><span class="o">=</span><span class="mi">64</span>
<span class="n">data_tensor</span> <span class="o">=</span> <span class="n">batchify</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">vocab</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">line_size</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">data_tensor</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">))</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1">#Create our model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">line_size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dembed</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">nheads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dk</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dv</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">nhidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">scheduler1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">scheduler2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">CosineAnnealingWarmRestarts</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">nepochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">print_frequency</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">min_loss</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">inf</span>
<span class="c1">#Train our model
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nepochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1e-1</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">min_loss</span><span class="p">:</span>
        <span class="n">min_loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="n">best_model</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">scheduler1</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler2</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Epoch : </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, loss : </span><span class="si">{</span><span class="n">min_loss</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="opening-the-magic-transformer-box">Opening the “magic transformer box”.</h1>

<p>In a sense, <em>the following is optional</em>. Many GPT models can be understood by considering transformers as black boxes and focusing on <em>how</em> they are trained. However, the model has a rich inner architecture, and it can be valuable to understand this architecture.</p>

<p>So, feel free to skip to the “Conclusion” section, but also feel free to continue reading.</p>

<h2 id="notation">Notation</h2>

<p>From now on, we will be dealing with tensors of order higher than two, as this simplifies the notation for transformers. Hence, we will use the Einstein notation, where statements such as \(\mathbf{a}_{ij} \mathbf{b}_j\) are equivalent to \(\sum_j \mathbf{a}_{ij} \mathbf{b}_j\). We will refer to elements of a tensor using lowercase bold letters, such as \(\mathbf{a}\), and the entire tensor using uppercase letters, such as \(\mathbf{A}\). Moreover, we will use a specific notation to refer to column and row vectors of a matrix. For example, if \(\mathbf{A}\) is a matrix (a second-order tensor), \(\mathbf{a}_{i \circ}\) refers to the \(i\)-th row vector of \(\mathbf{A}\), and \(\mathbf{a}_{\circ j}\) refers to the \(j\)-th column vector of \(\mathbf{A}\). Similarly, if \(\mathbf{B}\) is a third-order tensor, \(\mathbf{b}_{\circ j k}\) refers to the vector obtained by selecting indices \(j\) and \(k\) in the second and third positions, and \(\mathbf{b}_{i \circ \circ}\) refers to the submatrix obtained by selecting \(i\) in the first position. Furthermore, we use \(\mathbf{W}\) (with superscripts) to denote learnable parameters of our layer. Finally, \(\mathbf{1}_{\text{condition}}\) refers to the function that has a value of 1 if the <em>condition</em> is satisfied, and 0 otherwise. For example, \(\mathbf{1}_{i = j}\) is a function of \(i\) and \(j\) that equals 1 if \(i = j\) and 0 otherwise.</p>

<h2 id="embedding">Embedding</h2>

<p>Embedding is the operation of transforming sequences into vectors using a learnable lookup table. Suppose we have \(n_{vocab}\) items in our vocabulary and we want to associate each token with a \(d\)-dimensional vector. We set up a learnable matrix \(\mathbf{W}^e\) of size \((n_{vocab}, d)\), and for a sequence \(\mathbf{x}\) of size \(n\), the embedding of \(\mathbf{x}\) is given by a matrix \(\mathbf{V}\) of size \((n, d)\), where the \(i\)-th row is given by \(\mathbf{v}_{i \circ} = \mathbf{w}^e_{x_i \circ}\).</p>

<p>Next, we need information about the position of token \(x_i\). In the original transformers paper, this is achieved using a <em>fixed</em> vector \(p_i\), where the \(j\)-th value \(p_{i, j}\) is equal to \(\sin \frac{i}{10000^{j/d}}\) if \(j\) is even, and \(\cos \frac{i}{10000^{(j-1)/d}}\) if \(j\) is odd. This is combined with the token embedding by adding the positional embeddings to the corresponding token embeddings, resulting in</p>

\[\mathbf{v}_{i, \circ} = \mathbf{w}^e_{x_i \circ} + p_i.\]

<p>Alternatively, instead of using fixed positional embeddings, we can use <em>learned</em> positional embeddings for a context window of maximum size \(n_{ctx}\). In this case, we introduce a learnable matrix \(\mathbf{W}^{pos}\) of size \((n_{ctx}, d)\), and the positional embeddings are given by</p>

\[\mathbf{v}_{i\circ} = \mathbf{w}^e_{x_i \circ} + \mathbf{w}^{pos}_{i \circ}.\]

<p>The total number of parameters here will be either \(d n_{vocab}\), if positional embeddings are fixed, or \(d n_{vocab} + d n_{ctx}\), if positional embeddings are learned.</p>

<h2 id="unembedding">Unembedding</h2>

<p>The transformer operation, which will be explained in detail below, takes as input an embedding matrix \(\mathbf{V}\) of size \((n, d)\) and outputs another matrix \(\mathbf{V}'\) of the same size \((n, d)\). We need to associate each \(\mathbf{v}_{i \circ}\) with the probability \(p(x_{i+1}=u\mid \mathbf{x}^i)\) of the next token (at position \(i+1\)) having a value \(u \in [n_{vocab}]\). To achieve this, we:</p>

<ul>
  <li>Employ a linear layer with a weight matrix \(\mathbf{W}^u\) of size \((d, n_{vocab})\) and a bias vector \(\mathbf{b}^u\) of size \((n_{vocab})\). This transformation is given by \(\mathbf{v}_{ij}' = \mathbf{v}_{ik} \mathbf{w}^u_{kj} + \mathbf{b}^u_j\).</li>
  <li>Apply the softmax function to each row of the resulting matrix, yielding an output matrix \(\mathbf{Y}\) of size \((n, n_{vocab})\), where \(\mathbf{y}_{iu} = p(x_{i+1} = u\mid \mathbf{x}^i)\).</li>
</ul>

<p>The total number of parameters here will be \(d n_{vocab} + n_{vocab}\).</p>

<h2 id="attention">Attention</h2>

<p>Now we can delve into the core of the transformer architecture. Let’s start by considering the real matrix \(\mathbf{V}\) of dimensions \((n, d)\) obtained from embedding the sequence \(\mathbf{x}\) in \(d\) dimensions. An attention layer performs the operation</p>

\[\mathbf{v}_{ij} \to \mathbf{a}_{ik} \mathbf{v}_{kj},\]

<p>where \(\mathbf{A}\) is a matrix of dimensions \((n, n)\), such that \(\mathbf{a}_{ik} \geq 0\) and \(\sum_k \mathbf{a}_{ik} = 1\). In other words, the <em>attention matrix</em> \(\mathbf{A}\) computes the weighted mean \(\mathbf{a}_{ik} \mathbf{v}_{k \cdot}\) of the embedding vectors at other positions for each position \(i\).</p>

<p>The attention matrix \(\mathbf{A}\) is determined by two other matrices: the query matrix \(\mathbf{Q}\) and the key matrix \(\mathbf{C}\), both of dimensions \((n, d^q)\), where \(\mathbf{a}_{ik}\) informally measures the “similarity” between \(\mathbf{q}_{i\circ}\) and \(\mathbf{c}_{k\circ}\). In transformers, this similarity is defined by the “score function”</p>

\[\operatorname{score}(\mathbf{q}_{i \circ}, \mathbf{c}_{k \circ}) := \left&lt;\mathbf{q}_{i\cdot}, \mathbf{c}_{k\cdot}\right&gt;/\sqrt{d^q}.\]

<p>The idea is to measure similarity using an inner product, normalized by \(\sqrt{d^q}\) for stability. The score function is then transformed by applying the softmax function to each <em>row</em> of the score matrix, yielding</p>

\[\mathbf{a}_{ik} := \frac{e^{\operatorname{score}(\mathbf{q}_{i\circ}, \mathbf{c}_{k \circ})}}{\sum_k e^{\operatorname{score}(\mathbf{q}_{i\circ}, \mathbf{c}_{k \circ})}}.\]

<p>Therefore, the attention operation \(\operatorname{attn}\) is defined as</p>

\[\mathbf{V}' = \operatorname{attn}(\mathbf{V}, \mathbf{Q}, \mathbf{C}),\]

<p>where \(\mathbf{V}'\) has dimensions \((n, d)\).</p>

<p>Now, what are the query matrix and the key matrix? In self-attention, the query matrix \(\mathbf{Q}\) and the key matrix \(\mathbf{K}\) are derived from \(\mathbf{V}\) itself using learnable linear operations on each <em>row</em> of \(\mathbf{V}\), which represents each embedding vector. Specifically, we have</p>

\[\mathbf{q}_{ij} := \mathbf{v}_{il} \mathbf{w}^q_{lj},\]

<p>and</p>

\[\mathbf{c}_{ij} := \mathbf{v}_{il} \mathbf{w}^c_{lj},\]

<p>where \(\mathbf{W}^q\) and \(\mathbf{W}^c\) are both matrices of dimensions \(d \times d^q\) (with \(d^q &lt; d\) usually). Not only that, but we make the attention operation on a learnable projection of the embedding vectors of $\mathbf{v}_{ij}$ to a subspace of dimension $d^v$, using a matrix \(\mathbf{W}^v\) of dimensions \(d \times d^v\), such that we apply \(\mathbf{a}_{ik}\) to \(\mathbf{v}_{kl} \mathbf{w}^v_{lm}\). Of course, that results in a matrix of dimension $(n, d_v)$, so we must undo the projection using another learnable matrix \(\mathbf{W}^o\) of dimensions \(d^v \times d\) to return to dimension $(n, d)$. Consequently, the self-attention operation can be expressed as</p>

\[\mathbf{v}_{ij} \to \mathbf{a}_{ik} \mathbf{v}_{il} \mathbf{w}^v_{lm} \mathbf{w}^o_{mj},\]

<p>where</p>

<p>\(\mathbf{a}_{ik} =  \operatorname{softmax}_k \left[ \left&lt;\mathbf{v}_{il} \mathbf{w}^q_{l\cdot}, \mathbf{v}_{kl} \mathbf{w}^c_{l\cdot} \right&gt;/d' \right].\)
As a side note, notice that \(\mathbf{w}^v_{lm} \mathbf{w}^o_{mj}\) can be thought of as a linear operator operating on the embedding dimension of \(v\), of low-rank with \(d_v &lt; d\). We denote the operator as</p>

\[\mathbf{V}’ = \operatorname{sattn}(\mathbf{V};\mathbf{W}^q, \mathbf{W}^c, \mathbf{W}^v, \mathbf{W}^o).\]

<p>Yet, there is an essential consideration missing here. We want the output corresponding to the \(i\)-th token to depend only on tokens \(k \leq i\). The previous formulation does <em>not</em> ensure this because in general, \(\mathbf{a}_{ik}\) can be greater than zero even if \(k &gt; i\), meaning that the \(i\)-th position utilizes information from positions ahead. To address this, we need to introduce a <em>mask</em> in such cases to enforce \(\mathbf{a}_{ik} = 0\) if \(k &gt; i\). One approach is to modify the score function</p>

\[\operatorname{score}(\mathbf{q}_{i \circ}, \mathbf{c}_{k \circ})\]

<p>to a <em>causal</em> score function</p>

\[\operatorname{mscore}(\mathbf{q}_{i \circ}, \mathbf{c}_{k \circ}) = \operatorname{score}(\mathbf{q}_{i \circ}, \mathbf{c}_{k \circ}) \mathbf{1}_{k \leq i},\]

<p>which zeroes out elements where \(k &gt; i\). With this modification, we define the operator \(\operatorname{sattn}_m\) as</p>

\[\mathbf{V}’ = \operatorname{sattn}_m(\mathbf{V};\mathbf{W}^q, \mathbf{W}^c, \mathbf{W}^v, \mathbf{W}^o)\]

<p>to satisfy our requirement. Furthermore, in GPT-3, certain layers use a <em>banded causal mask</em>, where in addition to zeroing out elements \(k &gt; i\), elements where \(i - k &gt; \beta\) are also zeroed out. This ensures that each token only attends to a limited number of preceding elements. The figure below illustrates masked attention with no mask, a causal mask, and a banded causal mask.</p>

<p><img src="/assets/figs/lm1/attnmask2.png" alt="Attention with causal mask, banded mask, and banded causal mask" /></p>

<p>Masked attention with causal mask, banded mask, and banded causal mask.</p>

<p>All the formulations mentioned so far apply to a <em>single attention head</em>. However, the concept of multi-head attention reveals that we can repeat this process for \(n_h\) attention heads. Each attention head operates independently by having different learnable parameters. The outputs of all the attention heads are then summed up, resulting in the final output matrix. In this case, let</p>

\[\mathbf{W}^q, \mathbf{W}^c, \mathbf{W}^v, \mathbf{W}^o\]

<p>be order 3 tensors with dimensions</p>

\[(n_h, d, d^q), (n_h, d, d^q), (n_h, d, d^v), (n_h, d^v, d),\]

<p>respectively. Consequently, the multi-head attention operation \(\operatorname{sattn}_{m, h}\) is given by</p>

\[\mathbf{V’} = \operatorname{sattn}_{m, h}(\mathbf{V};\mathbf{W}^q, \mathbf{W}^c, \mathbf{W}^v, \mathbf{W}^o) = \sum_l \operatorname{sattn}_m(\mathbf{V};\mathbf{w}^q_{l \circ \circ}, \mathbf{w}^c_{l \circ \circ}, \mathbf{w}^v_{l \circ \circ}, \mathbf{w}^o_{l \circ \circ}).\]

<p><img src="/assets/figs/lm1/diagramattention.jpg" alt="Masked multi-head attention with two heads." /></p>

<p>Masked multi-head attention with two heads.</p>

<p>The implementation of masked multi-head attention is demonstrated below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dot_product_attn</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1">#queries : (..., ntokens, dk)
</span>    <span class="c1">#keys : (..., ntokens, dk)
</span>    <span class="c1">#values : (..., ntokens, dv)
</span>    <span class="c1">#mask : None or str or (..., ntokens, ntokens)
</span>
    <span class="n">dk</span> <span class="o">=</span> <span class="n">queries</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">inner_product</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ij, ...kj -&gt; ...ik'</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span> <span class="c1">#(..., ntokens, ntokens)
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">ntokens</span> <span class="o">=</span> <span class="n">values</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">mask</span> <span class="o">==</span> <span class="s">'upper'</span> <span class="ow">or</span> <span class="n">mask</span> <span class="o">==</span> <span class="s">'causal'</span><span class="p">:</span>
                <span class="n">maskbool</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ntokens</span><span class="p">,</span> <span class="n">ntokens</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">maskbool</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">NotImplementedError</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">mask</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">((</span><span class="o">~</span><span class="n">mask</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">))</span>
            <span class="n">inner_product</span> <span class="o">+=</span> <span class="n">mask</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">inner_product</span><span class="o">/</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(..., ntokens, ntokens)
</span>    <span class="n">wvalues</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ij, ...jk -&gt; ...ik'</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wvalues</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nheads</span> <span class="o">=</span> <span class="n">nheads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span> <span class="o">=</span> <span class="n">dmodel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">dk</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dv</span> <span class="o">=</span> <span class="n">dv</span>
        <span class="c1">#(..., ntokens, dmodel), (nheads, dmodel, dk) -&gt; (..., nheads, ntokens, dv)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">q_proj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="p">]))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">k_proj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="p">]))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v_proj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dv</span><span class="p">]))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">o_proj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dv</span><span class="p">]))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">initialize</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">q_proj_matrix</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k_proj_matrix</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v_proj_matrix</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">o_proj_matrix</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1">#queries : (..., ntokens, dmodel)
</span>        <span class="c1">#keys : (..., ntokens, dmodel)
</span>        <span class="c1">#values : (..., ntokens, dmodel)
</span>        <span class="c1">#mask : None or (..., ntokens, ntokens)
</span>        <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ij, kjm -&gt; ...kim'</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">q_proj_matrix</span><span class="p">)</span> <span class="c1">#(..., nheads, ntokens, dk)
</span>        <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ij, kjm -&gt; ...kim'</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">k_proj_matrix</span><span class="p">)</span> <span class="c1">#(..., nheads, ntokens, dk)
</span>        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ij, kjm -&gt; ...kim'</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">v_proj_matrix</span><span class="p">)</span> <span class="c1">#(..., nheads, ntokens, dv)
</span>        <span class="n">new_projected_values</span> <span class="o">=</span> <span class="n">dot_product_attn</span><span class="p">(</span><span class="n">projected_queries</span><span class="p">,</span> <span class="n">projected_keys</span><span class="p">,</span> <span class="n">projected_values</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="c1">#(..., nheads, ntokens, dv)
</span>        <span class="n">new_values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'...ijk, ilk -&gt; ...jl'</span><span class="p">,</span> <span class="n">new_projected_values</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">o_proj_matrix</span><span class="p">)</span> <span class="c1">#(..., ntokens, dmodel)
</span>        <span class="k">return</span> <span class="n">new_values</span>
</code></pre></div></div>

<p>Now, there are remaining components necessary to create a complete transformer block for our language model. These additional components are relatively straightforward compared to the attention block itself.</p>

<p>Before we proceed, it is important to note that only the attention mechanism exchanges information between token positions. We specifically designed the masked multi-head attention to meet the requirement of not using future information. If other components were to exchange information between tokens, we would need to ensure that they also adhere to this requirement or risk violating it.</p>

<h3 id="the-feedforward-neural-network">The Feedforward Neural Network</h3>

<p>The second major component of a transformer block is a feedforward neural network (FNN) that operates on a matrix \(\mathbf{V}\) of dimensions \((n, d)\). The FNN shares the same parameters across all positions in the matrix. It can be defined as a parameterized function \(f_{FNN}: \mathbb{R}^d \to \mathbb{R}^d\) that acts on each row \(\mathbf{v}_{i \circ}\) of \(\mathbf{V}\). The function consists of a feedforward neural network with a single hidden layer of dimension \(d^f\). In this formulation, the activation function \(\operatorname{act}\) is applied element-wise and is typically a rectified linear unit (ReLU) or a Gaussian Error Linear Unit (GELU). The learnable matrices \(\mathbf{W}^a\) and \(\mathbf{W}^b\) have dimensions \((d, d^f)\) and \((d^f, d)\), respectively. Additionally, there are learnable vectors \(\mathbf{b}^a\) and \(\mathbf{b}^b\) of size \((d^f, d)\). The FNN operation can be expressed as:</p>

\[f_{FNN}(\mathbf{V};\mathbf{W}^a, \mathbf{W}^b) = \operatorname{act}(\mathbf{v}_{ik} \mathbf{w}^a_{kl} + \mathbf{b}_l^a)\mathbf{w}^b_{kj} + \mathbf{b}_l^b.\]

<h3 id="layer-normalization-and-dropout">Layer Normalization and Dropout</h3>

<p>Layer normalization is an operation applied to each row \(\mathbf{v}_{i \circ}\) of the matrix \(\mathbf{V}\). It calculates the mean \(\mu_i\) and standard deviation \(\sigma_i\) of \(\mathbf{v}_{i \circ}\), normalizes \(\mathbf{v}_{i \circ}\) using \(\mu_i\) and \(\sigma_i\), and applies a learnable affine transformation shared across all positions. The normalization process involves computing the mean</p>

\[\mu_i = \frac{1}{d} \sum_{j} \mathbf{v}_{ij}\]

<p>and the standard deviation</p>

\[\sigma_i = \sqrt{\frac{1}{d} \sum_{j} (\mathbf{v}_{ij} - \mu_i)^2}.\]

<p>Parameters \(\mathbf{W}^{LN, a}\) and \(\mathbf{W}^{LN, b}\) of dimension \((d)\) are used for the affine transformation. Each element \(\mathbf{v}_{ij}\) is transformed as follows:</p>

\[\mathbf{v}_{ij} \to \mathbf{w}^{LN, a}_{j} \frac{\mathbf{v}_{ij} - \mu_i}{\sigma_i + \epsilon} + \mathbf{w}_j^{LN, b}.\]

<p>Similarly, dropout is a common operation used in neural networks during training. It randomly sets each input element to 0 with a probability of \(p\) and scales the remaining inputs by a factor of \(\frac{1}{1-p}\).</p>

<h3 id="putting-the-block-together">Putting the Block Together</h3>

<p>With all the components in place, we can now assemble the transformer block. First, we apply layer normalization to the input \(\mathbf{V}\) and then perform dropout on the output. These steps are applied to both the attention block \(\operatorname{block}_1\) and the feedforward neural network block \(\operatorname{block}_2\). We connect the blocks to the input \(\mathbf{V}\) using residual connections, which sum the output of each block with the input. The resulting transformer block is defined as:</p>

\[\operatorname{transblock}(\mathbf{V}) = \mathbf{V} + \operatorname{block}_1(\mathbf{V}) + \operatorname{block}_1(\operatorname{block}_2(\mathbf{V})).\]

<p>This formulation ensures that the output of the transformer block depends on the input and the outputs of both the attention and feedforward blocks. By stacking multiple transformer blocks together, we can create a deeper and more expressive model.</p>

<p>Below is an implementation of the transformer block, where the attention block is referred to as the “encoder block”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span> <span class="o">=</span> <span class="n">dmodel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dmodel</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dmodel</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">xnorm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nhidden</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">pdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">dk</span> <span class="k">if</span> <span class="n">dk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">dmodel</span>
        <span class="n">dv</span> <span class="o">=</span> <span class="n">dv</span> <span class="k">if</span> <span class="n">dv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">dmodel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mthattention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span> <span class="n">nhidden</span><span class="p">),</span>
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nhidden</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">pdrop</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># values = self.mthattention(values, values, values, mask)
</span>        <span class="n">self_attention</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">values</span> <span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">mthattention</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">self_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">values</span><span class="p">)))</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">values</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">values</span>
</code></pre></div></div>

<p><img src="/assets/figs/lm1/diagramblock.jpg" alt="A transformer decoder block" /></p>

<p>A transformer decoder block</p>

<p>Finally, it is useful to count the number of learnable parameters in a transformer block. Assuming that our multi-head attention has \(n_h\) heads, an input dimension of \(d\), value projection dimension of \(d^v\), and key/query dimension of \(d^q\), we can calculate the number of parameters in the multi-head attention as \(2n_h d(d^v + d^q)\). The feedforward neural network with a hidden layer of size \(d^f\) will have \(2d d^f + d + d^f\) parameters. Each layer normalization operation will have \(2d\) parameters. Considering these components, the total number of parameters in a transformer block is given by \(n_{block} = 2n_h d(d^v + d^q) + 2d d^f + d + d^f + 4d\). In GPT-3, the default values are typically set as follows: \(d\) is divisible by \(n_h\), \(d^v\) and \(d^q\) are both equal to \(d/n_h\), and \(d^f = 4d\). Under these default settings, the formula for the number of parameters simplifies to \(n_{block} = 12d^2 + 9d\).</p>

<h2 id="the-full-sequence-model">The Full Sequence Model</h2>

<p>To construct the full sequence model, we stack the following components sequentially: an embedding layer, positional embedding layer, \(n_T\) transformer blocks, a final layer normalization block, and an unembedding block. The architecture can be visualized as shown below:</p>

<p><img src="/assets/figs/lm1/diagramouter.jpg" alt="diagram3.jpg" /></p>

<p>The total number of parameters in the model can be calculated as:</p>

\[n_{block} = 2n_h d(d^v + d^q) + 2d d^f + d + d^f + 4d \\
n_{tr} = n_T n_{block} + 2d \\
n_{embed} = d n_{vocab} + d n_{ctx} \\
n_{unembed} = d n_{vocab} + n_{vocab} \\
n_{params} = n_{tr} + n_{embed} + n_{unembed},\]

<p>where \(n_{block}\) represents the number of parameters in a single transformer block, \(n_{tr}\) represents the total number of parameters in the transformer blocks, \(n_{embed}\) represents the number of parameters in the embedding layers, \(n_{unembed}\) represents the number of parameters in the unembedding layer. The term \(d n_{ctx}\) is dropped if a fixed positional embedding is used. Considering the default settings of GPT-3, we can simplify the formula to:</p>

\[n_{params} = n_T(12d^2 + 9d) + 2d + n_{vocab}(2d + 1) + d n_{ctx}.\]

<p>You can use the following code to count the number of parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">subsnone</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">val</span>

<span class="k">def</span> <span class="nf">number_of_gpt_parameters</span><span class="p">(</span><span class="n">nvocab</span><span class="p">,</span> <span class="n">dembed</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">nctx</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dk</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nhidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">nhidden</span> <span class="o">=</span> <span class="n">subsnone</span><span class="p">(</span><span class="n">nhidden</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">dembed</span><span class="p">)</span>
    <span class="n">dk</span> <span class="o">=</span> <span class="n">subsnone</span><span class="p">(</span><span class="n">dk</span><span class="p">,</span> <span class="n">dembed</span><span class="o">//</span><span class="n">nheads</span><span class="p">)</span>
    <span class="n">dv</span> <span class="o">=</span> <span class="n">subsnone</span><span class="p">(</span><span class="n">dv</span><span class="p">,</span> <span class="n">dembed</span><span class="o">//</span><span class="n">nheads</span><span class="p">)</span>
    <span class="n">n_attn</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">dembed</span><span class="o">*</span><span class="p">(</span><span class="n">dk</span> <span class="o">+</span> <span class="n">dv</span><span class="p">)</span><span class="o">*</span><span class="n">nheads</span>
    <span class="n">n_fnn</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">dembed</span><span class="o">*</span><span class="n">nhidden</span> <span class="o">+</span> <span class="n">dembed</span> <span class="o">+</span> <span class="n">nhidden</span>
    <span class="n">n_ln</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">dembed</span>
    <span class="n">n_transformer</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_attn</span> <span class="o">+</span> <span class="n">n_fnn</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">n_ln</span><span class="p">)</span><span class="o">*</span><span class="n">nlayers</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">dembed</span>
    <span class="n">n_embed</span> <span class="o">=</span> <span class="n">dembed</span><span class="o">*</span><span class="n">nvocab</span>
    <span class="k">if</span> <span class="n">nctx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">n_embed</span> <span class="o">+=</span> <span class="n">nctx</span><span class="o">*</span><span class="n">dembed</span>
    <span class="n">n_unembed</span> <span class="o">=</span> <span class="n">dembed</span><span class="o">*</span><span class="n">nvocab</span> <span class="o">+</span> <span class="n">nvocab</span>
    <span class="k">return</span> <span class="n">n_embed</span> <span class="o">+</span> <span class="n">n_unembed</span> <span class="o">+</span> <span class="n">n_transformer</span>
</code></pre></div></div>

<p>We can then finally comlpete our code for GPT.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
        Makes an encoder
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nhidden</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">pdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="n">nhidden</span><span class="p">,</span> <span class="n">pdrop</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ln</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">values</span>

<span class="k">class</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
        Implements a version of GPT-2

        Parameters:
        -----------
        nvocab: int
            Size of the model vocabulary
        nctx: int
            Maximum size of context window
        dembed: int
            Size of embedding dimension
        nlayers: int
            Number of encoder layers
        dk: Optional[int]
            Dimension of key projection. If None equals dembed.
        dv: Optional[int]
            Dimension of value projection. If None equals dembed
        nhidden: int
            Number of hidden layers in FNN part of encoder
        pdrop: float
            Dropout probability
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nvocab</span><span class="p">,</span> <span class="n">nctx</span><span class="p">,</span> <span class="n">dembed</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">nlayers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">nheads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dk</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nhidden</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nvocab</span> <span class="o">=</span> <span class="n">nvocab</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nctx</span> <span class="o">=</span> <span class="n">nctx</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">nctx</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">nvocab</span><span class="p">,</span> <span class="n">dembed</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">nctx</span><span class="p">,</span> <span class="n">dembed</span><span class="p">]))</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pos</span><span class="p">)</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">dk</span> <span class="k">if</span> <span class="n">dk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">dembed</span><span class="o">//</span><span class="n">nheads</span>
        <span class="n">dv</span> <span class="o">=</span> <span class="n">dv</span> <span class="k">if</span> <span class="n">dv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">dembed</span><span class="o">//</span><span class="n">nheads</span>
        <span class="n">nhidden</span> <span class="o">=</span> <span class="n">nhidden</span> <span class="k">if</span> <span class="n">nhidden</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mi">4</span><span class="o">*</span><span class="n">dembed</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">nlayers</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dembed</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="n">nhidden</span><span class="p">,</span> <span class="n">pdrop</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dembed</span><span class="p">,</span> <span class="n">nvocab</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">pdrop</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'mask'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nctx</span><span class="p">,</span> <span class="n">nctx</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="s">"""
            tokens : torch.Tensor
                Tensor of tokens, of type torch.long.
        """</span>
        <span class="c1">#tokens : (..., ntokens)
</span>        <span class="n">d</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">+</span> <span class="n">pos</span><span class="p">)</span> <span class="c1">#(..., ntokens, dembed)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="c1">#(..., ntokens, dembed)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(..., ntokens, dmodel)
</span>        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(..., ntokens, dmodel)
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(..., ntokens, dmodel)
</span>        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h1 id="conclusion">Conclusion.</h1>

<p>In real life, it is highly unlikely that you would train a language model from scratch. More commonly, you would choose a pretrained model from sources like <a href="https://huggingface.co/">HuggingFace</a> and fine-tune it to suit your specific task, or directly utilize it as it is. Alternatively, you could make use of existing language models like <a href="https://chat.openai.com/chat">ChatGPT</a>. Nevertheless, understanding the underlying mechanisms is valuable.</p>

<p>Is this the end of the story? For GPT-3, which serves as the foundation for models such as ChatGPT, it is almost the case. The key differences lie in:</p>

<ul>
  <li>
    <p><em>Scale</em>: The sheer scale of GPT-3 cannot be overstated. In the <a href="https://arxiv.org/pdf/2005.14165.pdf">original paper</a>, it boasts a vocabulary size of \(n_{vocab} = 50257\) and a context window size of \(n_{ctx} = 2048\). The largest model utilizes \(n_T = 96\), \(d=12288\), and \(n_h=96\), resulting in an astounding total number of parameters on the order of 175 billion (more precisely, estimated as \(n_{params} = 175223784799\)). Moreover, the training data is colossal, with the largest model trained on batches containing 3.8 million items and a total token count of 300 billion tokens, drawn from extensive sources like the <a href="https://www.wikipedia.org/">complete Wikipedia</a> or extensive web crawling databases like <a href="https://commoncrawl.org/">Common Crawl</a>. Training a model of this magnitude is a significant engineering endeavor, accompanied by training costs reaching millions of dollars.</p>
  </li>
  <li>
    <p><em>Sparse Attention Mask</em>: GPT-3 departs from using a full causal mask and instead incorporates a <em>banded</em> causal mask, alternating between the two. This enables efficient computation of the banded causal mask layers.</p>
  </li>
</ul>

<p>That concludes our exploration. You can access GPT-3 in the <a href="https://beta.openai.com/playground">OpenAI playground</a> using the <em>davinci</em> model. However, it is crucial to emphasize that <strong>GPT-3 is not ChatGPT</strong>. Let’s reiterate that: <strong>GPT-3 is not ChatGPT</strong>. To emphasize this point, let’s revisit the initial prompt in the opening paragraph, this time using pure GPT-3 <em>davinci</em>. The initial prompt is highlighted in bold.</p>

<blockquote>
  <p><strong>Chat, write to me a layman paragraph preceding a tutorial on language models, that motivates on why the reader should be interested in language models. Make it as engaging as possible.</strong></p>

  <p>This type of summary is called an abstract. It is an important tool for scientists and non-scientists alike to quickly grasp the essence of a research paper. If you are writing a research paper, it is a good idea to write an abstract yourself before writing a scientific paper.</p>

  <p>Preparation</p>

  <p>Go to a notebook and write down ideas or notes. For example, use the text “Deep Learning” to start. Alternatively, use the text “Language Models”. Then, write down relevant words and phrases that come to your mind.</p>

  <p>Write down a paragraph that motivates the reader to read further on language models.</p>

  <p>Write down a paragraph that summarizes the main idea about language models.</p>

  <p>Write down a paragraph that lists the main contributions of your paper.</p>

  <p>Write down a paragraph that lists the main assumptions of your work.</p>

  <p>Write down a paragraph that lists the main problems of your work.</p>

  <p>Write down a paragraph that lists the main lessons learned from your work.</p>

  <p>Write down a paragraph that lists the main open questions.</p>

  <p>Write down a paragraph that lists the main suggestions for future work.</p>

  <p>Write down a paragraph that lists the main references.</p>

  <p>Write down a paragraph</p>
</blockquote>

<p>Pure GPT-3 is capable of generating coherent text, but it lacks the ability to specifically address your requests. It resembles a highly intelligent child who possesses vast knowledge but lacks concern for fulfilling your specific desires.</p>

<p>In comparison, models like ChatGPT, and even more recent ones like <em>text-davinci-003</em>, excel at addressing user requests more effectively. ChatGPT can be viewed as a “tamed” version of GPT-3, achieved through two specific techniques: <em>supervised fine tuning</em> (SFT) and <em>reinforcement learning from human feedback</em> (RLHF). The upcoming second part of this discussion aims to explain this technique, bridging the gap between GPT-3 and ChatGPT.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 2]]></summary></entry><entry><title type="html">Hello World</title><link href="http://localhost:4000/2023/01/13/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Hello World" /><published>2023-01-13T16:11:16-03:00</published><updated>2023-01-13T16:11:16-03:00</updated><id>http://localhost:4000/2023/01/13/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/2023/01/13/welcome-to-jekyll.html"><![CDATA[<p>Hello. This is actually me beginning a personal website. I intend to blog on… whatever comes to my mind, really. Which lately is on AI alignment
and large language models. But whatever comes to my mind will be here.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Hello. This is actually me beginning a personal website. I intend to blog on… whatever comes to my mind, really. Which lately is on AI alignment and large language models. But whatever comes to my mind will be here.]]></summary></entry></feed>