<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>A technical tutorial on Large Language Models - Interlude on Reinforcement Learning | Danilo Naiff</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="A technical tutorial on Large Language Models - Interlude on Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Part 1" />
<meta property="og:description" content="Part 1" />
<link rel="canonical" href="http://localhost:4000/2023/01/26/llm-tutorial-2.html" />
<meta property="og:url" content="http://localhost:4000/2023/01/26/llm-tutorial-2.html" />
<meta property="og:site_name" content="Danilo Naiff" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-26T12:00:00-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A technical tutorial on Large Language Models - Interlude on Reinforcement Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-01-26T12:00:00-03:00","datePublished":"2023-01-26T12:00:00-03:00","description":"Part 1","headline":"A technical tutorial on Large Language Models - Interlude on Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/01/26/llm-tutorial-2.html"},"url":"http://localhost:4000/2023/01/26/llm-tutorial-2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Danilo Naiff" />

<!--
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
-->
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  },
  TeX: {
    Macros: {
      ket: ["\\left| #1 \\right\\rangle", 1],
      bra: ["\\left\\langle #1 \\right|", 1],
      braket: ["\\left\\langle #1 \\middle| #2 \\right\\rangle", 2],
      mean: ["\\left\\langle #1 \\right\\rangle", 1],
      abs: ["\\left| #1 \\right|", 1],
      norm: ["\\left\\| #1 \\right\\|", 1],
      trace: ["\\text{tr}\\left( #1 \\right)", 1],
      ptrace: ["\\text{tr}\_{#1}\\left( #2 \\right)", 2],
      pderiv: ["\\frac{\\partial #1}{\\partial #2}", 2],
      evalat: ["\\left. #1 \\right|_{#2}", 2],
    }
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script></head>
<body><header class="site-header">

  <div class="wrapper">
    <!--<a class="site-title" rel="author" href="/">Danilo Naiff</a>
    --><nav class="navbar navbar-expand-lg navbar-light bg-light">
      <a class="navbar-brand" href="/">Danilo Naiff</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
              Apps
            </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
              <a class="dropdown-item" href="/golf">Gravitational Golf</a>
              <a class="dropdown-item" href="/mbs">Quasielectrostatics</a>
            </div>
          </li>
        </ul>
      </div>
    </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A technical tutorial on Large Language Models - Interlude on Reinforcement Learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-01-26T12:00:00-03:00" itemprop="datePublished">
        Jan 26, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="/2023/01/14/llm-tutorial-1.html">Part 1</a></p>

<h1 id="update-2306">Update (23/06)</h1>

<p>I turned this into a general interlude for RL and RLHF, and I decided to write the next part of actually applying this to language models.</p>

<h1 id="update-2405">Update (24/05)</h1>

<p>This text has been revised and edited by ChatGPT 3.5 to improve grammar and overall structure.</p>

<h1 id="preamble">Preamble</h1>

<p>In Part 1, I mentioned that we would delve into ChatGPT. However, I must confess that I slightly misled you. The exact training details of ChatGPT have not been published yet. OpenAI has provided only a <a href="https://openai.com/blog/chatgpt/">general overview</a>. Nonetheless, OpenAI claims that the setup of ChatGPT is highly similar to the one used in the <a href="https://openai.com/blog/instruction-following/">InstructGPT</a> series. Therefore, we will rely on the information presented in the <a href="https://arxiv.org/pdf/2203.02155.pdf">original InstructGPT paper</a>. However, to comprehend InstructGPT, we first need to understand both supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Yet, to do that, we need to understand reinforcement learning, which is the point of this interlude.</p>

<h1 id="reinforcement-learning---a-primer">Reinforcement learning - A primer.</h1>

<p>To comprehend reinforcement learning from human feedback (RLHF), it is essential to grasp the fundamentals of reinforcement learning. For a more detailed introduction, refer to the <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">OpenAI tutorial</a>. Here, I provide a concise overview.</p>

<h2 id="the-building-blocks">The building blocks.</h2>

<p>Consider an agent that interacts with an environment through a sequence of steps. The environment begins in a state \(s_0 \in \mathcal{S}\) with probability \(p(s_0)\). At each step \(t\), the agent receives an observation \(o_t \in \mathcal{O}\) and a reward \(r_t\) based on a reward function of the form \(r_t = r(a_t, o_t)\). The agent takes an action \(a_t\), causing the environment to transition from state \(s_t\) to another state \(s_{t+1}\) with a probability \(p(s_{t+1} \mid s_t, a_t)\) (not necessarily known to the agent). The observation \(o_t\) is also dependent on the environment’s state, given by \(s_t \sim p(o_t \mid s_t)\). If \(o_t = s_t\), the environment is <em>fully observable</em>; otherwise, it is <em>partially observable</em>. Notably, if we have a prior \(p(s)\) on the state, we can expand \(p(o_{t+1} \mid o_t,a_t)\) as follows:</p>

\[p(o_{t+1} \mid o_t,a_t) = \int p(o_{t+1} \mid s_{t+1})p(s_{t+1} \mid a_t,o_t) ds_{t+1} \\
p(s_{t+1} \mid a_t,o_t) = \int p(s_{t+1} \mid a_t,s_t)p(s_t \mid o_t) ds_t \\
p(s_t \mid o_t) = \frac{p(o_t \mid s_t) p(s_t)}{\int p(o_t \mid s_t) p(s_t) ds_t}.\]

<p>The agent’s actions are guided by a (possibly probabilistic) <em>policy</em> \(\pi(a \mid o_t)\), where \(a_t \sim \pi(a_t \mid o_t)\). In the context of neural networks, policies are often denoted as \(\pi_\theta\), incorporating parameters. Let the agent begin at \(t=0\) and interact with the environment for \(T\) time steps until termination, constituting a single <em>episode</em>. The probability of the agent following a <em>trajectory</em> \(\tau = \{o_0, a_0, o_1, a_1, \ldots, o_T\}\) is given by</p>

\[p_{\pi}(\tau) = p(o_0) \prod_{i=1}^T p(o_{t+1} \mid o_t,a_t)\pi(a_t \mid o_t).\]

<p>A trajectory is associated with a cumulative reward \(R(\tau)\), which can be the cumulative sum for a <em>finite horizon</em> of \(T &lt; \infty\) time steps:</p>

\[R(\tau) = \sum_{t=0}^{T-1} \gamma^t r_t,\]

<p>where \(\gamma \in (0, 1]\) is the discount rate. Alternatively, for \(T=\infty\) and \(\gamma \leq 1\), the <em>infinite horizon</em> cumulative reward is given by</p>

\[R(\tau) = \sum_{t=0}^\infty \gamma^t r_t.\]

<p>It is important to note that an episode does not necessarily run indefinitely, and if it does, we must ensure \(\gamma &lt; 1\). We also define future cumulative rewards starting at time step \(t\) as follows:</p>

\[R_t(\tau) = \sum_{s=t}^{T-1} r_t, \quad R_t(\tau) = \sum_{s=t}^{\infty} \gamma^s r_t.\]

<p>Thus, for a policy \(\pi\), the expected cumulative reward \(J(\pi)\) is given by</p>

\[J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)}[R(\tau)].\]

<p>The <em>reinforcement learning objective</em> is to find a policy \(\pi(a \mid o)\) that maximizes \(J(\pi)\). In practice, we employ some form of gradient descent to minimize \(-J(\theta) = -J(\pi_\theta)\) for a parameterized policy \(\pi_\theta\), using an estimate of \(\nabla J(\theta)\). The next section, which delves into optimization algorithms, is optional as RLHF can be understood without exploring specific optimization algorithms. These algorithms typically employ minibatch gradient descent over trajectories.</p>

<h2 id="value-functions">Value functions.</h2>

<p>Before delving into optimizing \(J(\pi)\), let’s define some key components associated with the policy \(\pi(a \mid o)\). First, we have the <em>value function</em> \(V_\pi(o)\), which represents the expected cumulative reward of following policy \(\pi\) starting from observation \(o\). It is given by</p>

\[V_{\pi}(o) = \mathbb{E}_{\tau \sim p_\pi(\tau)}[R(\tau) \mid o_0=o],\]

<p>where \(R(\tau)\) is the cumulative reward obtained from trajectory \(\tau\). Similarly, the <em>action-value function</em> \(Q_{\pi}(o, a)\) represents the expected cumulative reward of following policy \(\pi\) after starting from observation \(o\) and taking action \(a\):</p>

\[Q_{\pi}(o,a) = \mathbb{E}_{\tau \sim p_\pi(\tau)}[R(\tau)  \mid o_0=o, a_0=a].\]

<p>We also have the relationship \(V_\pi(o) = \mathbb{E}_{a \sim p(a \mid o)}[Q_\pi(o,a)]\). For infinite time horizons, both \(V_\pi(o)\) and \(Q_\pi(o, a)\) satisfy the <em>Bellman equations</em>:</p>

\[V_\pi(o) = \mathbb{E}_{o', a \sim p(o' \mid o,a)\pi(a \mid o)}[r(a, o) + \gamma V_\pi(o')], \\
Q_\pi(o, a) = \mathbb{E}_{o' \sim p(o' \mid o,a)}[r(a, o) + \gamma \mathbb{E}_{a' \sim p(a' \mid o')}[Q_\pi(o',a')]],\]

<p>where \(\gamma\) is the discount factor. These equations capture the notion that the value or action/value from a state or action is the immediate expected reward plus the discounted future value of the expected future state. In finite time horizons, the situation becomes more complex as we need to consider the timing within the episode. However, by incorporating this information in our observations \(o_t\), the Bellman equations remain valid for finite horizons. Another important concept is the advantage function:</p>

\[A_\pi(o, a) = Q_\pi(o,a) - V_\pi(o),\]

<p>which quantifies the advantage of taking action \(a\) in state \(o\) followed by policy \(\pi\) compared to simply following policy \(\pi\) from state \(o\).</p>

<h2 id="solving-reinforcement-learning---offline">Solving reinforcement learning - offline.</h2>

<p>There are two main approaches to reinforcement learning: <em>online</em> reinforcement learning and <em>offline</em> reinforcement learning. In online RL, the agent updates its policy during the episode, similar to how humans and animals learn. However, in machine learning settings like ChatGPT, we typically employ <em>offline</em> RL. In offline RL, the agent plays multiple episodes \(\tau_i \sim p_{\pi_\theta}(\tau)\) using the current policy \(\pi_\theta\), observes the rewards \(\left(r^{(i)}_1, \ldots, r^{(i)}_{T} \right)\), and then updates \(J(\theta)\) using the stochastic approximation:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim p_{\pi}(\tau)}[R(\tau)] \approx \frac{1}{N} \sum_{i=1}^N R(\tau_i).\]

<p>To compute the gradient \(\nabla J(\theta)\), we encounter a challenge because the trajectories \(\tau_i\) are not directly differentiable with respect to the policy parameters \(\theta\). However, a clever trick allows us to overcome this issue. The derivation and more details can be found <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html">here</a>, but the key result is:</p>

\[\nabla J(\theta) = \mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)}\left[ \sum_{t=1}^{T} R(\tau) \nabla_\theta \log \pi_\theta(a_t \mid o_t)\right],\]

<p>which leads to the following approximation:</p>

\[\nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left[ \sum_{t=1}^{T} R(\tau_i) \nabla_\theta \log \pi_\theta(a^{(i)}_t \mid o^{(i)}_t)\right].\]

<p>In practice, we can substitute \(R(\tau_i)\) with \(R_t(\tau_i)\) and further replace \(R_t(\tau_i)\) with \(R_t(\tau_i) - b(s_t)\), where \(b(s_t)\) is a function that only depends on the state. We can approximate \(b(s_t)\) using a neural network \(V_\phi(s_t)\), which estimates the true value function \(V_{\pi_\theta}(s_t)\). The estimation is done by minimizing the following residual:</p>

\[V_\phi(s_t) = \argmin_\phi \sum_{i=1}^N \sum_{t=1}^T (V_\phi(s^{(i)}_t) - R_t(\tau_i))^2\]

<p>using stochastic gradient descent. These substitutions improve the stability of RL optimization. An annotated example of this algorithm applied to the CartPole environment can be found in <a href="https://colab.research.google.com/drive/1lz_FFlWFOexvU_LY80O5qqJGllPE_Ryo?usp=sharing">this notebook</a> I created earlier for other purposes.</p>

<p>The field of offline RL encompasses various algorithms, and in the case of InstructGPT, an algorithm called <em>proximal policy optimization</em> (PPO) is used. More details about PPO can be found <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">here</a>. However, the important point is not the specific RL algorithm being used, but rather understanding the underlying problem being addressed. This understanding will serve as the foundation for the next step: <em>reinforcement learning from human feedback</em>.</p>

<h1 id="from-rl-to-rlhf">From RL to RLHF.</h1>

<p>Having gained an understanding of reinforcement learning, we can now explore reinforcement learning from human feedback (RLHF). The key insight of RLHF is that the agent no longer has access to the true reward signal \(r_t\); instead, it must rely on a <em>model</em> of the reward function, which is obtained through feedback from humans. In this section, we will follow the explanation of RLHF provided by <a href="https://arxiv.org/pdf/1706.03741.pdf">Christiano et al.</a>.</p>

<p>Let’s consider that the agent has a model of the reward function \(\hat{r}(a, o)\), which is created based on human feedback. Sometimes, we use the subscript \(\phi\) (e.g., \(\hat{r}_\phi\)) to indicate that a neural network models the reward function. It’s important to note that \(\hat{r}\) represents a reward function that fits human preferences, rather than the true underlying human reward function, which can be more subjective.</p>

<p>We define the probability of a human preferring trajectory \(\tau\) over trajectory \(\tau'\) as:</p>

\[p(\tau \succ \tau' \mid \hat{r}) = \sigma(\hat{R}(\tau) - \hat{R}(\tau')),\]

<p>where \(\sigma\) is the sigmoid function. Similarly, we assume that \(p(\tau \prec \tau' \mid \hat{r}) = 1 - p(\tau \succ \tau' \mid \hat{r})\), meaning that we do not model indifference or non-comparability between trajectories.</p>

<p>Suppose we have a dataset of ordered pairs of trajectories \(\mathcal{D} = \{(\tau_i, \tau_i^\prime)\}_i\), where each pair \((\tau_i, \tau_i^\prime)\) represents a preference judgment made by a human (e.g., \(\tau_i \succ \tau_i^\prime\)). We construct our loss function for \(\hat{r} = \hat{r}_\phi\) as:</p>

\[l(\phi;\mathcal{D}) = \mathbb{E}_{(\tau, \tau^\prime) \sim \mathcal{D}}[-\log p(\tau \succ \tau' \mid \phi)].\]

<p>This loss is minimized using minibatch gradient descent, and the resulting learned reward function \(\hat{r}_\phi\) is used to update the agent’s policy \(\pi_\theta\) by maximizing \(J(\theta;\phi)\). This creates a cyclic process involving multiple steps:</p>

<ol>
  <li>Sample trajectories \(\mathcal{D}^u = \{\tau^u_k\}_k\) from the current policy \(\pi_\theta\).</li>
  <li>Collect human preferences based on \(\mathcal{D}^u\) to obtain trajectory preference pairs \(\mathcal{D} = \{(\tau_i, \tau_i’)\}_i\).</li>
  <li>Learn the reward function \(\hat{r}_\phi\) by minimizing the loss \(l(\phi;\mathcal{D})\) using minibatch gradient descent.</li>
  <li>Optimize the policy \(\pi_\theta\) using the learned reward function \(\hat{r}_\phi\) through a reinforcement learning algorithm that maximizes \(J(\pi;\hat{r}_\phi)\).</li>
  <li>Repeat from step 1.</li>
</ol>

<p>Next, we will use that general framework to explain instruct-based language models, and how they are crafted from base models.</p>

  </div><a class="u-url" href="/2023/01/26/llm-tutorial-2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Danilo Naiff&#39;s personal website.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/DFNaiff" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.linkedin.com/in/danilo-naiff" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://scholar.google.com/citations?user=UgDxpKgAAAAJ&hl" target="_blank" title="google_scholar">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#google_scholar"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
